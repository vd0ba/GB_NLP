{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тема «POS-tagger и NER»\n",
    "\n",
    "Задание 1. Написать теггер на данных с русским языком\n",
    "\n",
    "- проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
    "- написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "- сравнить все реализованные методы, сделать выводы\n",
    " \n",
    "Задание 2. Проверить, насколько хорошо работает NER\n",
    "\n",
    "Данные брать из http://www.labinform.ru/pub/named_entities/\n",
    "- проверить NER из nltk/spacy/deeppavlov.\n",
    "- написать свой NER, попробовать разные подходы.\n",
    "- передаём в сетку токен и его соседей.\n",
    "- передаём в сетку только токен.\n",
    "- свой вариант.\n",
    "- сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget -O ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "!wget -O ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pyconll\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pyconll.load_from_file('ru_syntagrus-ud-train.conllu')\n",
    "data_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in data_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24516, 8906, 8906)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdata_train), len(fdata_test), len(fdata_sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Unigram Tagger: 0.824,\n",
      "Bigram Tagger: 0.60939,\n",
      "Trigram Tagger: 0.178,\n",
      "Bigram and Unigram Tagger: 0.82928,\n",
      "Trigram, Bigram and Unigram Tagger: 0.82914,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_acc = unigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "bigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "trigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_unigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_bigram_unigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "print(f'Accuracy:\\nUnigram Tagger: {round(unigram_acc, 3)},\\nBigram Tagger: {round(bigram_acc, 5)},\\n'\n",
    "      f'Trigram Tagger: {round(trigram_acc, 3)},\\nBigram and Unigram Tagger: {round(bigram_unigram_acc, 5)},\\n'\n",
    "      f'Trigram, Bigram and Unigram Tagger: {round(trigram_bigram_unigram_acc, 5)},\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдаем лучший результат у комбинации \"Bigram and Unigram Tagger\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем свой вариант тагера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(' ' if tok[0] is None else tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.72 µs\n",
      "CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.91      0.91      0.91     15103\n",
      "         ADP       0.98      1.00      0.99     13717\n",
      "         ADV       0.90      0.90      0.90      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.88      0.98      0.93      5672\n",
      "         DET       0.89      0.73      0.80      4265\n",
      "        INTJ       0.35      0.29      0.32        24\n",
      "        NOUN       0.92      0.95      0.93     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.84      0.90      0.87      1734\n",
      "        PART       0.95      0.76      0.85      5125\n",
      "        PRON       0.83      0.90      0.86      7444\n",
      "       PROPN       0.75      0.58      0.66      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.80      0.91      0.85      2865\n",
      "         SYM       1.00      0.73      0.84        62\n",
      "        VERB       0.94      0.94      0.94     17110\n",
      "           X       0.51      0.15      0.23       134\n",
      "\n",
      "    accuracy                           0.93    153590\n",
      "   macro avg       0.85      0.80      0.81    153590\n",
      "weighted avg       0.93      0.93      0.92    153590\n",
      "\n",
      "TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.89      0.90      0.90     15103\n",
      "         ADP       0.99      0.99      0.99     13717\n",
      "         ADV       0.90      0.86      0.88      7783\n",
      "         AUX       0.82      0.97      0.89      1390\n",
      "       CCONJ       0.89      0.98      0.93      5672\n",
      "         DET       0.82      0.81      0.81      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.90      0.95      0.92     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.84      0.89      0.86      1734\n",
      "        PART       0.95      0.77      0.85      5125\n",
      "        PRON       0.88      0.85      0.86      7444\n",
      "       PROPN       0.77      0.51      0.62      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.91      0.86      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.93      0.92      0.93     17110\n",
      "           X       0.48      0.07      0.13       134\n",
      "\n",
      "    accuracy                           0.92    153590\n",
      "   macro avg       0.82      0.77      0.78    153590\n",
      "weighted avg       0.92      0.92      0.92    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.83      0.83      0.83     15103\n",
      "         ADP       0.97      0.98      0.98     13717\n",
      "         ADV       0.81      0.78      0.79      7783\n",
      "         AUX       0.81      0.96      0.88      1390\n",
      "       CCONJ       0.88      0.97      0.93      5672\n",
      "         DET       0.81      0.77      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.83      0.90      0.86     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.80      0.81      0.80      1734\n",
      "        PART       0.92      0.76      0.83      5125\n",
      "        PRON       0.84      0.87      0.85      7444\n",
      "       PROPN       0.69      0.44      0.53      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.80      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.88      0.83      0.85     17110\n",
      "           X       0.31      0.04      0.07       134\n",
      "\n",
      "    accuracy                           0.88    153590\n",
      "   macro avg       0.79      0.74      0.75    153590\n",
      "weighted avg       0.88      0.88      0.88    153590\n",
      "\n",
      "CountVectorizer(ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.45      0.60     15103\n",
      "         ADP       0.99      0.48      0.64     13717\n",
      "         ADV       0.91      0.77      0.83      7783\n",
      "         AUX       0.84      0.87      0.85      1390\n",
      "       CCONJ       0.89      0.20      0.33      5672\n",
      "         DET       0.85      0.63      0.72      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.98      0.71      0.82     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.87      0.54      0.67      1734\n",
      "        PART       0.96      0.73      0.83      5125\n",
      "        PRON       0.83      0.79      0.81      7444\n",
      "       PROPN       0.94      0.15      0.25      5473\n",
      "       PUNCT       0.38      1.00      0.55     29186\n",
      "       SCONJ       0.71      0.83      0.77      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.97      0.44      0.60     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.65    153590\n",
      "   macro avg       0.67      0.48      0.52    153590\n",
      "weighted avg       0.83      0.65      0.66    153590\n",
      "\n",
      "TfidfVectorizer(ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.41      0.57     15103\n",
      "         ADP       0.99      0.48      0.64     13717\n",
      "         ADV       0.94      0.78      0.85      7783\n",
      "         AUX       0.84      0.87      0.85      1390\n",
      "       CCONJ       0.88      0.20      0.33      5672\n",
      "         DET       0.91      0.61      0.73      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.98      0.65      0.78     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.88      0.55      0.68      1734\n",
      "        PART       0.97      0.73      0.83      5125\n",
      "        PRON       0.79      0.84      0.82      7444\n",
      "       PROPN       0.93      0.16      0.27      5473\n",
      "       PUNCT       0.37      1.00      0.54     29186\n",
      "       SCONJ       0.78      0.85      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.97      0.44      0.60     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.64    153590\n",
      "   macro avg       0.68      0.48      0.52    153590\n",
      "weighted avg       0.83      0.64      0.65    153590\n",
      "\n",
      "HashingVectorizer(n_features=1000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.41      0.20      0.27     15103\n",
      "         ADP       0.83      0.47      0.60     13717\n",
      "         ADV       0.56      0.62      0.59      7783\n",
      "         AUX       0.70      0.94      0.80      1390\n",
      "       CCONJ       0.84      0.18      0.30      5672\n",
      "         DET       0.50      0.52      0.51      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.25      0.53      0.34     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.39      0.43      0.41      1734\n",
      "        PART       0.85      0.74      0.79      5125\n",
      "        PRON       0.64      0.74      0.69      7444\n",
      "       PROPN       0.29      0.08      0.12      5473\n",
      "       PUNCT       0.00      0.00      0.00     29186\n",
      "       SCONJ       0.67      0.95      0.78      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.45      0.24      0.32     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.36    153590\n",
      "   macro avg       0.41      0.37      0.36    153590\n",
      "weighted avg       0.39      0.36      0.34    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.86      0.86      0.86     15103\n",
      "         ADP       0.97      0.99      0.98     13717\n",
      "         ADV       0.84      0.81      0.82      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.88      0.97      0.93      5672\n",
      "         DET       0.86      0.73      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.86      0.92      0.89     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.81      0.83      0.82      1734\n",
      "        PART       0.93      0.76      0.84      5125\n",
      "        PRON       0.83      0.89      0.86      7444\n",
      "       PROPN       0.71      0.41      0.52      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.89      0.87      0.88     17110\n",
      "           X       0.38      0.04      0.08       134\n",
      "\n",
      "    accuracy                           0.89    153590\n",
      "   macro avg       0.80      0.75      0.76    153590\n",
      "weighted avg       0.89      0.89      0.89    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.86      0.87      0.86     15103\n",
      "         ADP       0.98      0.99      0.98     13717\n",
      "         ADV       0.86      0.81      0.84      7783\n",
      "         AUX       0.81      0.96      0.88      1390\n",
      "       CCONJ       0.88      0.98      0.93      5672\n",
      "         DET       0.94      0.64      0.77      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.86      0.93      0.89     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.81      0.83      0.82      1734\n",
      "        PART       0.94      0.75      0.84      5125\n",
      "        PRON       0.79      0.94      0.86      7444\n",
      "       PROPN       0.73      0.40      0.52      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.82      0.90        62\n",
      "        VERB       0.88      0.89      0.88     17110\n",
      "           X       0.33      0.10      0.16       134\n",
      "\n",
      "    accuracy                           0.90    153590\n",
      "   macro avg       0.81      0.75      0.77    153590\n",
      "weighted avg       0.90      0.90      0.89    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.87      0.87     15103\n",
      "         ADP       0.97      0.99      0.98     13717\n",
      "         ADV       0.87      0.82      0.84      7783\n",
      "         AUX       0.81      0.96      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.84      0.76      0.80      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.87      0.93      0.90     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.83      0.85      0.84      1734\n",
      "        PART       0.93      0.76      0.84      5125\n",
      "        PRON       0.83      0.87      0.85      7444\n",
      "       PROPN       0.73      0.42      0.54      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.89      0.89      0.89     17110\n",
      "           X       0.55      0.04      0.08       134\n",
      "\n",
      "    accuracy                           0.90    153590\n",
      "   macro avg       0.82      0.75      0.77    153590\n",
      "weighted avg       0.90      0.90      0.90    153590\n",
      "\n",
      "HashingVectorizer(n_features=2000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.48      0.24      0.32     15103\n",
      "         ADP       0.89      0.47      0.62     13717\n",
      "         ADV       0.66      0.67      0.66      7783\n",
      "         AUX       0.75      0.94      0.84      1390\n",
      "       CCONJ       0.89      0.18      0.31      5672\n",
      "         DET       0.67      0.49      0.57      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.60      0.57      0.59     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.51      0.48      0.49      1734\n",
      "        PART       0.91      0.74      0.82      5125\n",
      "        PRON       0.68      0.83      0.75      7444\n",
      "       PROPN       0.37      0.09      0.15      5473\n",
      "       PUNCT       0.47      1.00      0.64     29186\n",
      "       SCONJ       0.76      0.90      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.54      0.29      0.38     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.58    153590\n",
      "   macro avg       0.51      0.44      0.44    153590\n",
      "weighted avg       0.61      0.58      0.55    153590\n",
      "\n",
      "HashingVectorizer(n_features=3000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.53      0.28      0.36     15103\n",
      "         ADP       0.92      0.47      0.62     13717\n",
      "         ADV       0.75      0.70      0.72      7783\n",
      "         AUX       0.77      0.94      0.85      1390\n",
      "       CCONJ       0.93      0.18      0.30      5672\n",
      "         DET       0.71      0.53      0.61      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.64      0.59      0.61     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.59      0.50      0.54      1734\n",
      "        PART       0.90      0.76      0.82      5125\n",
      "        PRON       0.74      0.81      0.78      7444\n",
      "       PROPN       0.42      0.12      0.18      5473\n",
      "       PUNCT       0.46      1.00      0.63     29186\n",
      "       SCONJ       0.73      0.95      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.59      0.32      0.42     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.59    153590\n",
      "   macro avg       0.54      0.45      0.46    153590\n",
      "weighted avg       0.64      0.59      0.57    153590\n",
      "\n",
      "HashingVectorizer(n_features=5000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.60      0.31      0.41     15103\n",
      "         ADP       0.92      0.48      0.63     13717\n",
      "         ADV       0.80      0.71      0.75      7783\n",
      "         AUX       0.79      0.95      0.86      1390\n",
      "       CCONJ       0.91      0.19      0.31      5672\n",
      "         DET       0.70      0.67      0.69      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.69      0.61      0.64     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.63      0.49      0.55      1734\n",
      "        PART       0.91      0.74      0.82      5125\n",
      "        PRON       0.79      0.78      0.78      7444\n",
      "       PROPN       0.52      0.14      0.22      5473\n",
      "       PUNCT       0.44      1.00      0.61     29186\n",
      "       SCONJ       0.75      0.88      0.81      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.66      0.35      0.46     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.60    153590\n",
      "   macro avg       0.56      0.46      0.47    153590\n",
      "weighted avg       0.67      0.60      0.59    153590\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "vectorizers = [CountVectorizer(ngram_range=(1, 3), analyzer='char'), \n",
    "               TfidfVectorizer(ngram_range=(1, 3), analyzer='char'), \n",
    "               HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=1000)] \n",
    "vectorizers_word = [CountVectorizer(ngram_range=(1, 3), analyzer='word'), \n",
    "               TfidfVectorizer(ngram_range=(1, 3), analyzer='word'), \n",
    "               HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=1000)] \n",
    "n_features = [2000, 3000, 5000]\n",
    "vectorizers_hash = [HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=feat) for feat in n_features]\n",
    "vectorizers_hash_word = [HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=feat) for feat in n_features]\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for vectorizer in vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word:\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok)\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "    f1 = f1_score(test_enc_labels, pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    acc = accuracy_score(test_enc_labels, pred)\n",
    "    accuracy_scores.append(acc)\n",
    "    \n",
    "    print(vectorizer)\n",
    "    print(classification_report(test_enc_labels, pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.924069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.897959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.892972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.890910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.876592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.662481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.650386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range...</td>\n",
       "      <td>0.585894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range...</td>\n",
       "      <td>0.568223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range...</td>\n",
       "      <td>0.547066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
       "      <td>0.340580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Vectorizer  f1_score\n",
       "0   CountVectorizer(analyzer='char', ngram_range=(...  0.924069\n",
       "1   TfidfVectorizer(analyzer='char', ngram_range=(...  0.917500\n",
       "8   HashingVectorizer(analyzer='char', n_features=...  0.897959\n",
       "7   HashingVectorizer(analyzer='char', n_features=...  0.892972\n",
       "6   HashingVectorizer(analyzer='char', n_features=...  0.890910\n",
       "2   HashingVectorizer(analyzer='char', n_features=...  0.876592\n",
       "3                 CountVectorizer(ngram_range=(1, 3))  0.662481\n",
       "4                 TfidfVectorizer(ngram_range=(1, 3))  0.650386\n",
       "11  HashingVectorizer(n_features=5000, ngram_range...  0.585894\n",
       "10  HashingVectorizer(n_features=3000, ngram_range...  0.568223\n",
       "9   HashingVectorizer(n_features=2000, ngram_range...  0.547066\n",
       "5   HashingVectorizer(n_features=1000, ngram_range...  0.340580"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'f1_score': f1_scores})\n",
    "result_model.sort_values('f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.925822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.919917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.901524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.897057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.894511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.879510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.653936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.640152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range...</td>\n",
       "      <td>0.604213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range...</td>\n",
       "      <td>0.592747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range...</td>\n",
       "      <td>0.575812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
       "      <td>0.359861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Vectorizer  Accuracy\n",
       "0   CountVectorizer(analyzer='char', ngram_range=(...  0.925822\n",
       "1   TfidfVectorizer(analyzer='char', ngram_range=(...  0.919917\n",
       "8   HashingVectorizer(analyzer='char', n_features=...  0.901524\n",
       "7   HashingVectorizer(analyzer='char', n_features=...  0.897057\n",
       "6   HashingVectorizer(analyzer='char', n_features=...  0.894511\n",
       "2   HashingVectorizer(analyzer='char', n_features=...  0.879510\n",
       "3                 CountVectorizer(ngram_range=(1, 3))  0.653936\n",
       "4                 TfidfVectorizer(ngram_range=(1, 3))  0.640152\n",
       "11  HashingVectorizer(n_features=5000, ngram_range...  0.604213\n",
       "10  HashingVectorizer(n_features=3000, ngram_range...  0.592747\n",
       "9   HashingVectorizer(n_features=2000, ngram_range...  0.575812\n",
       "5   HashingVectorizer(n_features=1000, ngram_range...  0.359861"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model_acc = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'Accuracy': accuracy_scores})\n",
    "result_model_acc.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.9/site-packages (22.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (65.5.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (0.37.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.9/site-packages (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/site-packages (from spacy) (1.21.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/site-packages (from spacy) (1.9.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/site-packages (from spacy) (4.63.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -m venv .env\n",
    "!source .env/bin/activate\n",
    "!pip3 install -U pip setuptools wheel\n",
    "!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting ru-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.4.0/ru_core_news_sm-3.4.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.9/site-packages (from ru-core-news-sm==3.4.0) (3.4.2)\n",
      "Requirement already satisfied: pymorphy2>=0.9 in /usr/local/lib/python3.9/site-packages (from ru-core-news-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.9/site-packages (from pymorphy2>=0.9->ru-core-news-sm==3.4.0) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.9/site-packages (from pymorphy2>=0.9->ru-core-news-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.9/site-packages (from pymorphy2>=0.9->ru-core-news-sm==3.4.0) (2.4.417127.4579844)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.21.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.63.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (8.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.1.1)\n",
      "Installing collected packages: ru-core-news-sm\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed ru-core-news-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!/usr/local/bin/spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!unzip collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corus\n",
    "from corus import load_ne5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/dv/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /Users/dv/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/dv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dv/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Жириновский предлагает обменять с США Сноудена на Бута\\r\\n\\r\\nЛидер ЛДПР Владимир Жириновский предложил обменять бывшего сотрудника ЦРУ США Эдварда Сноудена, который прибыл в Москву, на осужденного в Америке бизнесмена Виктора Бута.\\r\\n\\r\\n\"Сноудена ни в коем случае не высылать в США, а обменять на Виктора Бута и Константина Ярошенко. В идеале — добавить генерала Олега Калугина\", — написал он в своем микроблоге в Twitter.\\r\\n\\r\\nСноуден, работавший на компанию Booz Allen Hamilton — подрядчика Центрального разведывательного управления США, в начале июня распространил секретный ордер суда, по которому спецслужбы получили доступ ко всем звонкам крупнейшего сотового оператора Verizon, а также данные о сверхсекретной программе агентства национальной безопасности PRISM, позволяющей отслеживать электронные коммуникации на крупнейших сайтах. В воскресенье стало известно, что Сноуден прибыл из Гонконга в Москву и запросил убежища в Эквадоре.\\r\\n\\r\\nЧто ждет Эдварда Сноудена\\r\\n\\r\\nЭдвард Сноуден, наверное, не знал только одного: что отныне от него ничего уже не будет зависеть. Москва-Гавана-Каракас – в новой траектории жизни. В плотном кольце новых друзей, которым нужно быстро вытащить из тебя то, что еще не сказал. А может, говорить больше и нечего. Когда и они в этом убедятся, как раз объявят посадку в Каракасе. Добро пожаловать в третий мир. Потому что если тебе нет место в первом, не станет с тобой надолго связываться и второй. Подробнее >>\\r\\n\\r\\n\\r\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = load_ne5('Collection5/')\n",
    "document = next(records).text\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Hamilton', 'PERSON'),\n",
       " ('Америке', 'PERSON'),\n",
       " ('Виктора Бута', 'PERSON'),\n",
       " ('Москву', 'PERSON'),\n",
       " ('Сноуден', 'PERSON'),\n",
       " ('Эдварда Сноудена', 'PERSON'),\n",
       " ('Эдварда Сноудена Эдвард Сноуден', 'PERSON')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Жириновский\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " предлагает обменять с \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сноудена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " на Бута\r",
       "</br>\r",
       "</br>Лидер \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ЛДПР\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Владимир Жириновский\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " предложил обменять бывшего сотрудника \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ЦРУ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Эдварда Сноудена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", который прибыл в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Москву\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", на осужденного в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Америке\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " бизнесмена \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Виктора Бута\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ".\r",
       "</br>\r",
       "</br>&quot;\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сноудена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " ни в коем случае не высылать в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", а обменять на \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Виктора Бута\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " и \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Константина Ярошенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ". В идеале — добавить генерала \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Олега Калугина\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "&quot;, — написал он в своем микроблоге в \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".\r",
       "</br>\r",
       "</br>\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сноуден\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", работавший на компанию \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Booz Allen Hamilton\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " — подрядчика \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Центрального разведывательного управления\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", в начале июня распространил секретный ордер суда, по которому спецслужбы получили доступ ко всем звонкам крупнейшего сотового оператора \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Verizon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", а также данные о сверхсекретной программе агентства национальной безопасности PRISM, позволяющей отслеживать электронные коммуникации на крупнейших сайтах. В воскресенье стало известно, что \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сноуден\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " прибыл из \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Гонконга\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Москву\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " и запросил убежища в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Эквадоре\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".\r",
       "</br>\r",
       "</br>Что ждет \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Эдварда Сноудена\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "\r",
       "</br>\r",
       "</br>\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Эдвард Сноуден\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", наверное, не знал только одного: что отныне от него ничего уже не будет зависеть. \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Москва-Гавана-Каракас\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " – в новой траектории жизни. В плотном кольце новых друзей, которым нужно быстро вытащить из тебя то, что еще не сказал. А может, говорить больше и нечего. Когда и они в этом убедятся, как раз объявят посадку в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Каракасе\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Добро пожаловать в третий мир. Потому что если тебе нет место в первом, не станет с тобой надолго связываться и второй. Подробнее &gt;&gt;\r",
       "</br>\r",
       "</br>\r",
       "</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ny_bb = document\n",
    "article = nlp(ny_bb)\n",
    "displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Жириновский PROPN nsubj\n",
      "предлагает VERB ROOT\n",
      "обменять VERB xcomp\n",
      "с ADP case\n",
      "США PROPN obl\n",
      "Сноудена PROPN obj\n",
      "на ADP case\n",
      "Бута PROPN obl\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "Лидер NOUN nsubj\n",
      "ЛДПР PROPN nmod\n",
      "Владимир PROPN appos\n",
      "Жириновский PROPN flat:name\n",
      "предложил VERB conj\n",
      "обменять VERB xcomp\n",
      "бывшего ADJ amod\n",
      "сотрудника NOUN obj\n",
      "ЦРУ PROPN nmod\n",
      "США PROPN nmod\n",
      "Эдварда PROPN appos\n",
      "Сноудена PROPN flat:name\n",
      ", PUNCT punct\n",
      "который PRON nsubj\n",
      "прибыл VERB acl:relcl\n",
      "в ADP case\n",
      "Москву PROPN obl\n",
      ", PUNCT punct\n",
      "на ADP case\n",
      "осужденного NOUN acl\n",
      "в ADP case\n",
      "Америке PROPN obl\n",
      "бизнесмена NOUN appos\n",
      "Виктора PROPN appos\n",
      "Бута PROPN flat:name\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "\" PUNCT punct\n",
      "Сноудена PROPN obj\n",
      "ни PART advmod\n",
      "в ADP fixed\n",
      "коем DET fixed\n",
      "случае NOUN fixed\n",
      "не PART advmod\n",
      "высылать VERB ROOT\n",
      "в ADP case\n",
      "США PROPN obl\n",
      ", PUNCT punct\n",
      "а CCONJ cc\n",
      "обменять VERB conj\n",
      "на ADP case\n",
      "Виктора PROPN obl\n",
      "Бута PROPN flat:name\n",
      "и CCONJ cc\n",
      "Константина PROPN conj\n",
      "Ярошенко PROPN flat:name\n",
      ". PUNCT punct\n",
      "В ADP case\n",
      "идеале NOUN ROOT\n",
      "— PUNCT punct\n",
      "добавить VERB parataxis\n",
      "генерала NOUN obj\n",
      "Олега PROPN appos\n",
      "Калугина PROPN flat:name\n",
      "\" PUNCT punct\n",
      ", PUNCT punct\n",
      "— PUNCT punct\n",
      "написал VERB parataxis\n",
      "он PRON nsubj\n",
      "в ADP case\n",
      "своем DET det\n",
      "микроблоге NOUN obl\n",
      "в ADP case\n",
      "Twitter PROPN nmod\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "Сноуден PROPN nsubj\n",
      ", PUNCT punct\n",
      "работавший VERB acl\n",
      "на ADP case\n",
      "компанию NOUN obl\n",
      "Booz X appos\n",
      "Allen X flat:foreign\n",
      "Hamilton X flat:foreign\n",
      "— PUNCT punct\n",
      "подрядчика NOUN appos\n",
      "Центрального ADJ amod\n",
      "разведывательного ADJ amod\n",
      "управления NOUN nmod\n",
      "США PROPN nmod\n",
      ", PUNCT punct\n",
      "в ADP case\n",
      "начале NOUN obl\n",
      "июня NOUN nmod\n",
      "распространил VERB ROOT\n",
      "секретный ADJ amod\n",
      "ордер NOUN obj\n",
      "суда NOUN nmod\n",
      ", PUNCT punct\n",
      "по ADP case\n",
      "которому PRON obl\n",
      "спецслужбы NOUN nsubj\n",
      "получили VERB acl:relcl\n",
      "доступ NOUN obj\n",
      "ко ADP case\n",
      "всем DET det\n",
      "звонкам NOUN nmod\n",
      "крупнейшего ADJ amod\n",
      "сотового ADJ amod\n",
      "оператора NOUN nmod\n",
      "Verizon PROPN appos\n",
      ", PUNCT punct\n",
      "а CCONJ cc\n",
      "также ADV fixed\n",
      "данные NOUN conj\n",
      "о ADP case\n",
      "сверхсекретной ADJ amod\n",
      "программе NOUN nmod\n",
      "агентства NOUN nmod\n",
      "национальной ADJ amod\n",
      "безопасности NOUN nmod\n",
      "PRISM PROPN appos\n",
      ", PUNCT punct\n",
      "позволяющей VERB acl\n",
      "отслеживать VERB xcomp\n",
      "электронные ADJ amod\n",
      "коммуникации NOUN obj\n",
      "на ADP case\n",
      "крупнейших ADJ amod\n",
      "сайтах NOUN obl\n",
      ". PUNCT punct\n",
      "В ADP case\n",
      "воскресенье NOUN obl\n",
      "стало VERB ROOT\n",
      "известно ADJ xcomp\n",
      ", PUNCT punct\n",
      "что SCONJ mark\n",
      "Сноуден PROPN nsubj\n",
      "прибыл VERB ccomp\n",
      "из ADP case\n",
      "Гонконга PROPN obl\n",
      "в ADP case\n",
      "Москву PROPN obl\n",
      "и CCONJ cc\n",
      "запросил VERB conj\n",
      "убежища NOUN obj\n",
      "в ADP case\n",
      "Эквадоре PROPN obl\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "Что PRON obj\n",
      "ждет VERB ROOT\n",
      "Эдварда PROPN nsubj\n",
      "Сноудена PROPN flat:name\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "Эдвард PROPN appos\n",
      "Сноуден PROPN flat:name\n",
      ", PUNCT punct\n",
      "наверное ADV parataxis\n",
      ", PUNCT punct\n",
      "не PART advmod\n",
      "знал VERB conj\n",
      "только PART advmod\n",
      "одного NUM obj\n",
      ": PUNCT punct\n",
      "что PRON nsubj\n",
      "отныне ADV advmod\n",
      "от ADP case\n",
      "него PRON obl\n",
      "ничего PRON nsubj\n",
      "уже ADV advmod\n",
      "не PART advmod\n",
      "будет AUX aux\n",
      "зависеть VERB parataxis\n",
      ". PUNCT punct\n",
      "Москва PROPN nsubj\n",
      "- PROPN nsubj\n",
      "Гавана PROPN nsubj\n",
      "- PROPN nsubj\n",
      "Каракас PROPN nsubj\n",
      "– PUNCT punct\n",
      "в ADP case\n",
      "новой ADJ amod\n",
      "траектории NOUN ROOT\n",
      "жизни NOUN nmod\n",
      ". PUNCT punct\n",
      "В ADP case\n",
      "плотном ADJ amod\n",
      "кольце NOUN ROOT\n",
      "новых ADJ amod\n",
      "друзей NOUN nmod\n",
      ", PUNCT punct\n",
      "которым PRON iobj\n",
      "нужно ADJ acl:relcl\n",
      "быстро ADV advmod\n",
      "вытащить VERB csubj\n",
      "из ADP case\n",
      "тебя PRON obl\n",
      "то PRON obj\n",
      ", PUNCT punct\n",
      "что SCONJ nsubj\n",
      "еще ADV advmod\n",
      "не PART advmod\n",
      "сказал VERB acl\n",
      ". PUNCT punct\n",
      "А CCONJ cc\n",
      "может VERB parataxis\n",
      ", PUNCT punct\n",
      "говорить VERB csubj\n",
      "больше ADV advmod\n",
      "и CCONJ cc\n",
      "нечего VERB ROOT\n",
      ". PUNCT punct\n",
      "Когда SCONJ mark\n",
      "и PART advmod\n",
      "они PRON nsubj\n",
      "в ADP case\n",
      "этом PRON obl\n",
      "убедятся VERB ROOT\n",
      ", PUNCT punct\n",
      "как ADV advmod\n",
      "раз NOUN fixed\n",
      "объявят VERB advcl\n",
      "посадку NOUN obj\n",
      "в ADP case\n",
      "Каракасе PROPN obl\n",
      ". PUNCT punct\n",
      "Добро NOUN ROOT\n",
      "пожаловать VERB nmod\n",
      "в ADP case\n",
      "третий ADJ amod\n",
      "мир NOUN obl\n",
      ". PUNCT punct\n",
      "Потому ADV mark\n",
      "что SCONJ fixed\n",
      "если SCONJ mark\n",
      "тебе PRON iobj\n",
      "нет VERB advcl\n",
      "место NOUN nsubj\n",
      "в ADP case\n",
      "первом ADJ nmod\n",
      ", PUNCT punct\n",
      "не PART advmod\n",
      "станет VERB ROOT\n",
      "с ADP case\n",
      "тобой PRON obl\n",
      "надолго ADV advmod\n",
      "связываться VERB xcomp\n",
      "и CCONJ cc\n",
      "второй ADJ conj\n",
      ". PUNCT punct\n",
      "Подробнее ADV ROOT\n",
      "> PUNCT punct\n",
      "> PUNCT punct\n",
      "\n",
      "\n",
      "\n",
      " SPACE dep\n"
     ]
    }
   ],
   "source": [
    "for token in article:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        \n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUT         219014\n",
       "PER          21178\n",
       "ORG          13641\n",
       "LOC           4564\n",
       "GEOPOLIT      4349\n",
       "MEDIA         2481\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input, Bidirectional,Reshape\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn import model_selection, preprocessing, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 13:56:33.426834: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(2048)\n",
    "valid_data = valid_data.batch(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_len\n",
    "    )\n",
    "\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.unique(encoder.inverse_transform(valid_y),return_counts=True)[1]\n",
    "t=t/t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  tf.keras.layers.Embedding(len(vectorize_layer.get_vocabulary()), 64, mask_zero=True),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dense(300, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(50, activation='relu'),\n",
    "  tf.keras.layers.Dense(6, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "98/98 [==============================] - 5s 40ms/step - loss: 0.7432 - accuracy: 0.8219 - val_loss: 0.3929 - val_accuracy: 0.8624\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.2662 - accuracy: 0.9139 - val_loss: 0.2424 - val_accuracy: 0.9312\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 0.1511 - accuracy: 0.9556 - val_loss: 0.2098 - val_accuracy: 0.9407\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 0.1218 - accuracy: 0.9624 - val_loss: 0.2079 - val_accuracy: 0.9412\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 0.1127 - accuracy: 0.9640 - val_loss: 0.2081 - val_accuracy: 0.9418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19ecc5400>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',         \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GEOPOLIT', 0.9684594529900095),\n",
       " ('LOC', 0.9577035941135312),\n",
       " ('MEDIA', 0.9768202204155387),\n",
       " ('ORG', 1.0044458789699269),\n",
       " ('OUT', 1.0168183409614737),\n",
       " ('PER', 0.8422694103030541)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(encoder.classes_,pred.mean(axis=0)/t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(len(vectorize_layer.get_vocabulary()), 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(6,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "98/98 [==============================] - 24s 200ms/step - loss: 0.8631 - accuracy: 0.8209 - val_loss: 0.4129 - val_accuracy: 0.8601\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 20s 207ms/step - loss: 0.3062 - accuracy: 0.9019 - val_loss: 0.2673 - val_accuracy: 0.9156\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 19s 194ms/step - loss: 0.1940 - accuracy: 0.9379 - val_loss: 0.2360 - val_accuracy: 0.9343\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 18s 181ms/step - loss: 0.1557 - accuracy: 0.9549 - val_loss: 0.2179 - val_accuracy: 0.9402\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 17s 178ms/step - loss: 0.1367 - accuracy: 0.9601 - val_loss: 0.2122 - val_accuracy: 0.9414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a33047f0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gJABxhalLVQu",
    "IaQMCGHFLVQ6",
    "5AJk1B39LVRP",
    "RJlvqWuALVRs",
    "rck5OVqhLVSA",
    "mV3fmzp-LVSU",
    "H5THCOjMLVSg",
    "02s2Vh7MLVSj",
    "b1khxRFDLVSm",
    "sfUmWcAQLVSt",
    "BxvtN-3zLVS5",
    "gyrHhYkgLVTB"
   ],
   "name": "sem1_intro_common.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
