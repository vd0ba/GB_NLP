{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –£—Ä–æ–∫ 14. Transfer learning\n",
    "\n",
    "  1. –≤–∑—è—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑\n",
    "    https://www.kaggle.com/datasets/mrapplexz/bashim-quotes \\\n",
    "    –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–∏—Ö —Ü–∏—Ç–∞—Ç\n",
    "\n",
    "  2. –≤–∑—è—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑\n",
    "    https://github.com/natasha/corus \\\n",
    "    load_lenta2 \\\n",
    "    –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —Å–∞–º —Ç–µ–∫—Å—Ç –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ \\\n",
    "    –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å T5/ –∏–ª–∏ GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–µ–π\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers corus sentencepiece -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-08 12:38:26.101376: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!unzip \"data/archive.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-30 11:24:00+00:00</td>\n",
       "      <td>22010.0</td>\n",
       "      <td>&lt;Ares&gt; ppdv, –≤—Å–µ —é–Ω–∏–∫—Å—ã –æ—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã.. –æ–Ω–∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-30 11:25:00+00:00</td>\n",
       "      <td>25105.0</td>\n",
       "      <td>&lt;—Ç–æ–º–∞—Ç–∏–∫_—Ä–∞–¥&gt; –∞ —Ç—ã –Ω–µ —á—É–≤—Å—Ç–≤—É–µ—à—å –∫—Ä–∞—Å–æ—Ç—É –º–∏—Ä–∞?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-30 11:27:00+00:00</td>\n",
       "      <td>7192.0</td>\n",
       "      <td>&lt;–î–æ—Ä&gt; \"–º—ã—à–∫–∞, –ø–æ—á–µ–º—É —É —Ç–µ–±—è —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –≥–ª–∞–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-30 11:28:00+00:00</td>\n",
       "      <td>29169.0</td>\n",
       "      <td>&lt;PPDV[os2]&gt; \"–ú–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ –±–æ–ª—å–Ω—ã–µ, –±–µ–≥–∞—Ç—å ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004-08-30 11:26:00+00:00</td>\n",
       "      <td>7140.0</td>\n",
       "      <td>&lt;Ohtori_Akio&gt; –º—ã - –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ - –∂–∏–≤—ë–º —Å ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date   rating  \\\n",
       "id                                      \n",
       "1  2004-08-30 11:24:00+00:00  22010.0   \n",
       "2  2004-08-30 11:25:00+00:00  25105.0   \n",
       "3  2004-08-30 11:27:00+00:00   7192.0   \n",
       "4  2004-08-30 11:28:00+00:00  29169.0   \n",
       "5  2004-08-30 11:26:00+00:00   7140.0   \n",
       "\n",
       "                                                 text  \n",
       "id                                                     \n",
       "1   <Ares> ppdv, –≤—Å–µ —é–Ω–∏–∫—Å—ã –æ—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã.. –æ–Ω–∏...  \n",
       "2   <—Ç–æ–º–∞—Ç–∏–∫_—Ä–∞–¥> –∞ —Ç—ã –Ω–µ —á—É–≤—Å—Ç–≤—É–µ—à—å –∫—Ä–∞—Å–æ—Ç—É –º–∏—Ä–∞?...  \n",
       "3   <–î–æ—Ä> \"–º—ã—à–∫–∞, –ø–æ—á–µ–º—É —É —Ç–µ–±—è —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –≥–ª–∞–∑...  \n",
       "4   <PPDV[os2]> \"–ú–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ –±–æ–ª—å–Ω—ã–µ, –±–µ–≥–∞—Ç—å ...  \n",
       "5   <Ohtori_Akio> –º—ã - –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ - –∂–∏–≤—ë–º —Å ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATASET_PATH) as f:\n",
    "     df = pd.read_json(DATASET_PATH, lines=True).set_index('id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Ares&gt; ppdv, –≤—Å–µ —é–Ω–∏–∫—Å—ã –æ—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã.. –æ–Ω–∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;—Ç–æ–º–∞—Ç–∏–∫_—Ä–∞–¥&gt; –∞ —Ç—ã –Ω–µ —á—É–≤—Å—Ç–≤—É–µ—à—å –∫—Ä–∞—Å–æ—Ç—É –º–∏—Ä–∞?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–î–æ—Ä&gt; \"–º—ã—à–∫–∞, –ø–æ—á–µ–º—É —É —Ç–µ–±—è —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –≥–ª–∞–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PPDV[os2]&gt; \"–ú–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ –±–æ–ª—å–Ω—ã–µ, –±–µ–≥–∞—Ç—å ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;Ohtori_Akio&gt; –º—ã - –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ - –∂–∏–≤—ë–º —Å ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "id                                                   \n",
       "1   <Ares> ppdv, –≤—Å–µ —é–Ω–∏–∫—Å—ã –æ—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã.. –æ–Ω–∏...\n",
       "2   <—Ç–æ–º–∞—Ç–∏–∫_—Ä–∞–¥> –∞ —Ç—ã –Ω–µ —á—É–≤—Å—Ç–≤—É–µ—à—å –∫—Ä–∞—Å–æ—Ç—É –º–∏—Ä–∞?...\n",
       "3   <–î–æ—Ä> \"–º—ã—à–∫–∞, –ø–æ—á–µ–º—É —É —Ç–µ–±—è —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –≥–ª–∞–∑...\n",
       "4   <PPDV[os2]> \"–ú–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ –±–æ–ª—å–Ω—ã–µ, –±–µ–≥–∞—Ç—å ...\n",
       "5   <Ohtori_Akio> –º—ã - –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ - –∂–∏–≤—ë–º —Å ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['date', 'rating'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 608/608 [00:00<00:00, 136kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.71M/1.71M [00:00<00:00, 2.06MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.27M/1.27M [00:00<00:00, 1.54MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 551M/551M [00:48<00:00, 11.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***\n",
      "xxx: –ß–æ –∫–∞–∫?\n",
      "yyy: –î–∞ –≤–æ—Ç, –Ω–∞ —Ä–∞–±–æ—Ç–µ –∑–∞–∫—Ä—ã–ª–∏ –¥–æ—Å—Ç—É–ø –∫ –≤–∫, —Å–∏–∂—É —Ç–µ–ø–µ—Ä—å –Ω–∞ hh\n",
      "***\n",
      "<–ü–∏–Ω–≥> –±–ª—è, –º–Ω–µ –¥–∞–∂–µ –≤ sims2 –Ω–µ –¥–∞—é—Ç!\n",
      "***\n",
      "xxx: –í –∞—Ä–º–∏–∏ –∂ –∫–∞–∫: –∫—Ä—É–≥–ª–æ–µ - –Ω–µ—Å—É—Ç, –∞ –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–µ - –∫–∞—Ç—è—Ç. \n",
      "xxx: –í–æ—Ç —Å—Ç–æ—é –∏ —Å–º–æ—Ç—Ä—é –Ω–∞ —Å–Ω–µ–≥–æ–≤–∏–∫–∞ –≤ –Ω–∞—à–µ–π —á–∞—Å—Ç–∏. –ö–≤–∞–¥—Ä–∞—Ç–Ω—ã–π. \n",
      "xxx: –í—Å–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ - –Ω–∞–∫–∞—Ç–∞–ª–∏.\n",
      "***\n",
      "< mary.j > –æ–¥–µ–ª–∞ —è —Å–µ–≥–æ–¥–Ω—è —Å–≤–æ—é —á—ë—Ä–Ω—É—é –∫–æ—Ñ—Ç–æ—á–∫—É —Å –¥–µ–∫–æ–ª—å—Ç–µ, –Ω—É –∏ –∏–¥—É –ø–ª–∞—Ç–∏—Ç—å –∑–∞ –∫—Ä–µ–¥–∏—Ç. –ò–¥—É –≤–Ω–∏–∑ –Ω–∞ —Å–∞–∫—Å–∞–≥–∞–Ω—Å–∫–æ–≥–æ —Å—Ç–æ—è—Ç 3 –ø–∞—Ä–Ω—è –∏ –ª—É–ø—è—Ç—Å—è –Ω–∞ –º–æ–∏ —Å–∏—Å—å–∫—Ç :\"–≤–æ—Ç —ç—Ç–æ –¥–∞\", \"–∫—Ä—É—Ç–æ\"... –ò–¥—É –Ω–∞–≤–µ—Ä—Ö –æ–Ω–∏ –∂–µ –∏ —Ç–æ –∂–µ —Å–∞–º–æ–µ. –Ø \"–º–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ —Å–∏—Å–µ–∫ –Ω–µ –≤–∏–¥–∏–ª–∏?\" . –û–¥–∏–Ω –∏–∑ –Ω–∏—Ö : \"–¥–∞ –æ–±—ã—á–Ω–æ –ª–∏–±–æ —Å–∏—Å—å–∫–∏ –ª–∏–±–æ –ª–∏—Ü–æ. –ê –í–∞–º —Ç–∞–∫ –∫ –ª–∏—Ü—É –í–∞—à–∏ —Å–∏—Å—å–∫–∏\".\n",
      "***\n",
      "- –∞–≤–∞—Ç–∞—Ä —É —Ç–µ–±—è –∫–ª–µ–≤—ã–π :)\n",
      "- –∞–≥–∞!!! –∞ —á—Ç–æ —ç—Ç–æ?\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sep = '\\n***\\n'\n",
    "prefix = sep.join([''] + random.sample(list(df['text']), k=5) + [''])\n",
    "tokens = tokenizer(prefix, return_tensors='pt')\n",
    "tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "end_token_id = tokenizer.encode('***')[0]\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "size = tokens['input_ids'].shape[1]\n",
    "output = model.generate(\n",
    "    **tokens, \n",
    "    do_sample=False, \n",
    "    max_length=size+50, \n",
    "    repetition_penalty=5., \n",
    "    temperature=0.5,\n",
    "    num_beams=10,\n",
    ")\n",
    "decoded = tokenizer.decode(output[0])\n",
    "result = decoded[len(prefix):]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df.loc[:10000, 'text'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def build_text_files(data_json, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    for texts in data_json:\n",
    "        summary = str(texts).strip()\n",
    "        summary = re.sub(r\"\\[\\w+\\]\", \"\", summary)\n",
    "        summary = re.sub(r\"<[\\w+,\\!, -]>\", \"\", summary)\n",
    "        summary = re.sub(r\"<\\w+>\", \"\", summary)\n",
    "        summary = re.sub(r\"\\s\", \" \", summary)\n",
    "        data += summary + \"  \"\n",
    "    f.write(data)\n",
    "  \n",
    "build_text_files(train,'./train_dataset.txt')\n",
    "build_text_files(test,'./test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1666\n",
      "Test dataset length: 294\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset length: \"+ str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_path = './train_dataset.txt'\n",
    "test_path = './test_dataset.txt'\n",
    "\n",
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./GPT/gpt2-train\", \n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4,  \n",
    "    eval_steps = 400, \n",
    "    save_steps=800, \n",
    "    warmup_steps=500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 676\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 507\n",
      "  Number of trainable parameters = 125231616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [507/507 53:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.273200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=507, training_loss=4.267856063692292, metrics={'train_runtime': 3227.2923, 'train_samples_per_second': 0.628, 'train_steps_per_second': 0.157, 'total_flos': 132475060224000.0, 'train_loss': 4.267856063692292, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./GPT/gpt2-train\n",
      "Configuration saved in ./GPT/gpt2-train/config.json\n",
      "Model weights saved in ./GPT/gpt2-train/pytorch_model.bin\n",
      "tokenizer config file saved in GPT/gpt2-train/tokenizer_config.json\n",
      "Special tokens file saved in GPT/gpt2-train/special_tokens_map.json\n",
      "Configuration saved in GPT/model_gpt2/config.json\n",
      "Model weights saved in GPT/model_gpt2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained('GPT/gpt2-train')\n",
    "model.save_pretrained('GPT/model_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file GPT/model_gpt2/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"GPT/model_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file GPT/model_gpt2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at GPT/model_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GPT/gpt2-train\")\n",
    "model_new = AutoModelForCausalLM.from_pretrained(\"GPT/model_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ia http://www.livejournal.com/users/maxim_korolev/126565.html#cutid1  <@Luftwaffe> –£ –º–µ–Ω—è –µ—Å—Ç—å –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–µ, —á—Ç–æ –∫—Ç–æ-—Ç–æ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç —Å–æ –º–Ω–æ–π –ø–æ–≥–æ–≤–æ—Ä–∏—Ç—å –ø–æ –¥—É—à–∞–º... (—Å) * Viktor has quit IRC (Viktor is now known as viktor_officer) [Quit]\n"
     ]
    }
   ],
   "source": [
    "size = tokens['input_ids'].shape[1]\n",
    "output = model_new.generate(\n",
    "    **tokens, \n",
    "    do_sample=False, \n",
    "    max_length=size+100, \n",
    "    repetition_penalty=5., \n",
    "    temperature=0.5,\n",
    "    num_beams=10,\n",
    ")\n",
    "decoded = tokenizer.decode(output[0])\n",
    "result = decoded[len(prefix):]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LentaRecord(\n",
       "    url='https://lenta.ru/news/1914/09/16/hungarnn/',\n",
       "    title='1914. –†—É—Å—Å–∫–∏–µ –≤–æ–π—Å–∫–∞ –≤—Å—Ç—É–ø–∏–ª–∏ –≤\\xa0–ø—Ä–µ–¥–µ–ª—ã –í–µ–Ω–≥—Ä–∏–∏  ',\n",
       "    text='–ë–æ–∏ —É –°–æ–ø–æ—Ü–∫–∏–Ω–∞ –∏ –î—Ä—É—Å–∫–µ–Ω–∏–∫ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–µ–º –≥–µ—Ä–º–∞–Ω—Ü–µ–≤. –ù–µ–ø—Ä–∏—è—Ç–µ–ª—å, –ø—Ä–∏–±–ª–∏–∑–∏–≤—à–∏—Å—å —Å —Å–µ–≤–µ—Ä–∞ –∫ –û—Å–æ–≤—Ü—É –Ω–∞—á–∞–ª –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫—É—é –±–æ—Ä—å–±—É —Å –∫—Ä–µ–ø–æ—Å—Ç—å—é. –í –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏–π—Å–∫–æ–º –±–æ—é –ø—Ä–∏–Ω–∏–º–∞—é—Ç —É—á–∞—Å—Ç–∏–µ —Ç—è–∂–µ–ª—ã–µ –∫–∞–ª–∏–±—Ä—ã. –° —Ä–∞–Ω–Ω–µ–≥–æ —É—Ç—Ä–∞ 14 —Å–µ–Ω—Ç—è–±—Ä—è –æ–≥–æ–Ω—å –¥–æ—Å—Ç–∏–≥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è. –ü–æ–ø—ã—Ç–∫–∞ –≥–µ—Ä–º–∞–Ω—Å–∫–æ–π –ø–µ—Ö–æ—Ç—ã –ø—Ä–æ–±–∏—Ç—å—Å—è –±–ª–∏–∂–µ –∫ –∫—Ä–µ–ø–æ—Å—Ç–∏ –æ—Ç—Ä–∞–∂–µ–Ω–∞. –í –ì–∞–ª–∏—Ü–∏–∏ –º—ã –∑–∞–Ω—è–ª–∏ –î–µ–º–±–∏—Ü—É. –ë–æ–ª—å—à–∞—è –∫–æ–ª–æ–Ω–Ω–∞, –æ—Ç—Å—Ç—É–ø–∞–≤—à–∞—è –ø–æ —à–æ—Å—Å–µ –æ—Ç –ü–µ—Ä–µ–º—ã—à–ª—è –∫ –°–∞–Ω–æ–∫—É, –æ–±—Å—Ç—Ä–µ–ª–∏–≤–∞–ª–∞—Å—å —Å –≤—ã—Å–æ—Ç –Ω–∞—à–µ–π –±–∞—Ç–∞—Ä–µ–µ–π –∏ –±–µ–∂–∞–ª–∞, –±—Ä–æ—Å–∏–≤ –ø–∞—Ä–∫–∏, –æ–±–æ–∑ –∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏. –í—ã–ª–∞–∑–∫–∏ –≥–∞—Ä–Ω–∏–∑–æ–Ω–∞ –ü–µ—Ä–µ–º—ã—à–ª—è –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑—É—Å–ø–µ—à–Ω—ã–º–∏. –ü—Ä–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–µ–º—Å—è –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏–∏ –∞–≤—Å—Ç—Ä–∏–π—Ü–µ–≤ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –∏—Ö —á–∞—Å—Ç–µ–π, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç—Å—è –Ω–æ–≤—ã–µ –ø–∞—Ä—Ç–∏–∏ –ø–ª–µ–Ω–Ω—ã—Ö, –æ—Ä—É–¥–∏—è –∏ –ø—Ä–æ—á–∞—è –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å. –ù–∞ –ø–µ—Ä–µ–≤–∞–ª–µ –£–∂–æ–∫ –º—ã —Ä–∞–∑–±–∏–ª–∏ –Ω–µ–ø—Ä–∏—è—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—Ä—è–¥, –≤–∑—è–ª–∏ –µ–≥–æ –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—é –∏ –º–Ω–æ–≥–æ –ø–ª–µ–Ω–Ω—ã—Ö –∏, –ø—Ä–æ–¥–æ–ª–∂–∞—è –ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞—Ç—å, –≤—Å—Ç—É–ø–∏–ª–∏ –≤ –ø—Ä–µ–¥–µ–ª—ã –í–µ–Ω–≥—Ä–∏–∏. ¬´–†—É—Å—Å–∫–∏–π –∏–Ω–≤–∞–ª–∏–¥¬ª, 16 —Å–µ–Ω—Ç—è–±—Ä—è 1914 –≥–æ–¥–∞.',\n",
       "    topic='–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞',\n",
       "    tags='–ü–µ—Ä–≤–∞—è –º–∏—Ä–æ–≤–∞—è',\n",
       "    date=datetime.datetime(1914, 9, 16, 0, 0)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from corus import load_lenta2\n",
    "\n",
    "path = 'data/lenta-ru-news.csv.bz2'\n",
    "records = load_lenta2(path)\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1914. –ü—Ä–∞–∑–¥–Ω–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–µ—Ç–∏—è –ú.–Æ. –õ–µ—Ä–º–æ–Ω—Ç–æ–≤–∞ –æ—Ç...</td>\n",
       "      <td>–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è, –≤ –≤–∏–¥—É –ø—Ä–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1914. Das ist Nesteroff!</td>\n",
       "      <td>–®—Ç–∞–±—Å-–∫–∞–ø–∏—Ç–∞–Ω –ü. –ù. –ù–µ—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –¥–Ω—è—Ö, —É–≤–∏–¥–µ–≤ –≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1914. –ë—É–ª—å–¥–æ–≥-–≥–æ–Ω–µ—Ü –ø–æ–¥ –õ—å–µ–∂–µ–º</td>\n",
       "      <td>–§–æ—Ç–æ–≥—Ä–∞—Ñ-–∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç Daily Mirror —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1914. –ü–æ–¥ –õ—é–±–ª–∏–Ω–æ–º –ø–æ–π–º–∞–Ω —à–≤–∞–±—Å–∫–∏–π –∑–≤–µ—Ä—å</td>\n",
       "      <td>–õ–∏—Ü–∞, –ø—Ä–∏–µ—Ö–∞–≤—à–∏–µ –≤ –í–∞—Ä—à–∞–≤—É –∏–∑ –õ—é–±–ª–∏–Ω–∞, –ø–µ—Ä–µ–¥–∞—é...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ö–æ—Å–º–æ–Ω–∞–≤—Ç—ã —Å–æ–º–Ω–µ–≤–∞—é—Ç—Å—è –≤¬†–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ \"–ú–∏—Ä–∞\"</td>\n",
       "      <td>–ö–∞–∫ —Å—Ç–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –∞–≥–µ–Ω—Ç—Å—Ç–≤—É –ê—Å—Å–æ—à–∏—ç–π—Ç–µ–¥ –ü—Ä–µ—Å—Å...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  1914. –ü—Ä–∞–∑–¥–Ω–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–µ—Ç–∏—è –ú.–Æ. –õ–µ—Ä–º–æ–Ω—Ç–æ–≤–∞ –æ—Ç...   \n",
       "1                           1914. Das ist Nesteroff!   \n",
       "2                    1914. –ë—É–ª—å–¥–æ–≥-–≥–æ–Ω–µ—Ü –ø–æ–¥ –õ—å–µ–∂–µ–º    \n",
       "3           1914. –ü–æ–¥ –õ—é–±–ª–∏–Ω–æ–º –ø–æ–π–º–∞–Ω —à–≤–∞–±—Å–∫–∏–π –∑–≤–µ—Ä—å   \n",
       "4         –ö–æ—Å–º–æ–Ω–∞–≤—Ç—ã —Å–æ–º–Ω–µ–≤–∞—é—Ç—Å—è –≤¬†–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ \"–ú–∏—Ä–∞\"   \n",
       "\n",
       "                                                text  \n",
       "0  –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è, –≤ –≤–∏–¥—É –ø—Ä–æ...  \n",
       "1  –®—Ç–∞–±—Å-–∫–∞–ø–∏—Ç–∞–Ω –ü. –ù. –ù–µ—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –¥–Ω—è—Ö, —É–≤–∏–¥–µ–≤ –≤...  \n",
       "2  –§–æ—Ç–æ–≥—Ä–∞—Ñ-–∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç Daily Mirror —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞...  \n",
       "3  –õ–∏—Ü–∞, –ø—Ä–∏–µ—Ö–∞–≤—à–∏–µ –≤ –í–∞—Ä—à–∞–≤—É –∏–∑ –õ—é–±–ª–∏–Ω–∞, –ø–µ—Ä–µ–¥–∞—é...  \n",
       "4  –ö–∞–∫ —Å—Ç–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –∞–≥–µ–Ω—Ç—Å—Ç–≤—É –ê—Å—Å–æ—à–∏—ç–π—Ç–µ–¥ –ü—Ä–µ—Å—Å...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(record.title, record.text) for record in records]\n",
    "df_news = pd.DataFrame({'title': [record[0] for record in data], 'text': [record[1] for record in data]})\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800974, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_news[:2000], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['title', 'text'],\n",
       "     num_rows: 1600\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'text'],\n",
       "     num_rows: 400\n",
       " }))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ú–∏–Ω–∏—Å—Ç—Ä –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –†–æ—Å—Å–∏–∏ –í–ª–∞–¥–∏–º–∏—Ä –†—É—à–∞–π–ª–æ —Å—á–∏—Ç–∞–µ—Ç, —á—Ç–æ —á–µ—á–µ–Ω—Å–∫–∏–π –º—É—Ñ—Ç–∏–π –ê—Ö–º–µ–¥ –ö–∞–¥—ã—Ä–æ–≤ –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å \"–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π —Ñ–∏–≥—É—Ä–æ–π\" –ø—Ä–∏ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–∞—Ö –º–µ–∂–¥—É —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–º —Ü–µ–Ω—Ç—Ä–æ–º –∏ –ß–µ—á–Ω–µ–π –ø–æ —É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é —Å–∏—Ç—É–∞—Ü–∏–∏ –≤ —Ä–µ—Å–ø—É–±–ª–∏–∫–µ. –û–± —ç—Ç–æ–º –æ–Ω —Å–æ–æ–±—â–∏–ª –≤ —á–µ—Ç–≤–µ—Ä–≥ –≤ –±–µ—Å–µ–¥–µ —Å –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∞–º–∏, –ø–µ—Ä–µ–¥–∞–µ—Ç –ò–¢–ê–†-–¢–ê–°–°. –ì–ª–∞–≤–∞ –ú–í–î –Ω–∞–ø–æ–º–Ω–∏–ª, —á—Ç–æ –ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –∏ –º—É—Ñ—Ç–∏–π –ö–∞–¥—ã—Ä–æ–≤ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –Ω–∞–∫–∞–Ω—É–Ω–µ –∏ –ø—Ä–æ–≤–µ–ª–∏ —É—Å–ø–µ—à–Ω—ã–π –¥–∏–∞–ª–æ–≥. –†—É—à–∞–π–ª–æ —Å —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ–º –æ—Ç–º–µ—Ç–∏–ª, —á—Ç–æ \"–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ª–æ–≥ —É–∂–µ –Ω–∞—á–∞–ª—Å—è\", –æ–¥–Ω–∞–∫–æ –æ–Ω –ø–æ–¥—á–µ—Ä–∫–Ω—É–ª, —á—Ç–æ \"–º—ã –ø–æ–∫–∞ –Ω–µ –¥–µ–ª–∞–µ–º –¥–∞–ª–µ–∫–æ –∏–¥—É—â–∏—Ö –≤—ã–≤–æ–¥–æ–≤\". –†—É—à–∞–π–ª–æ –≤ –æ—á–µ—Ä–µ–¥–Ω–æ–π —Ä–∞–∑ –ø–æ–¥—á–µ—Ä–∫–Ω—É–ª, —á—Ç–æ –ê—Å–ª–∞–Ω –ú–∞—Å—Ö–∞–¥–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —á–µ—á–µ–Ω—Å–∫—É—é —Å—Ç–æ—Ä–æ–Ω—É –Ω–∞ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–∞—Ö. –ù–∞–ø–æ–º–Ω–∏–º, —á—Ç–æ 11 –æ–∫—Ç—è–±—Ä—è –ú–∞—Å—Ö–∞–¥–æ–≤ –∑–∞—è–≤–∏–ª, —á—Ç–æ –æ–Ω –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç –æ—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –º—É—Ñ—Ç–∏—è —á–µ—á–µ–Ω—Å–∫–æ–π —Ä–µ—Å–ø—É–±–ª–∏–∫–∏ –ê—Ö–º–µ–¥–∞-–•–∞–¥–∂–∏ –ö–∞–¥—ã—Ä–æ–≤–∞. –ú–∞—Å—Ö–∞–¥–æ–≤ –æ–±—ä—è—Å–Ω–∏–ª —Ç–æ–≥–¥–∞ —Å–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ (–≤–µ—Å—å–º–∞ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —à–∞—Ä–∏–∞—Ç–∞) —Ç–µ–º, —á—Ç–æ –º—É—Ñ—Ç–∏–π \"–ø—ã—Ç–∞–µ—Ç—Å—è —Ä–∞–∑–≤—è–∑–∞—Ç—å –≥—Ä–∞–∂–¥–∞–Ω—Å–∫—É—é –≤–æ–π–Ω—É –≤ –ß–µ—á–Ω–µ\".'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/spiece.model\n",
      "loading file tokenizer.json from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/tokenizer_config.json\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 145.75ba/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 143.74ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"IlyaGusev/rut5_base_sum_gazeta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_len_text = max(map(lambda txt: len(txt.split()), dataset_train['text']))\n",
    "max_len_tlt = max(map(lambda txt: len(txt.split()), dataset_train['title']))\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized_input = tokenizer(batch['text'], padding='max_length', truncation=True, max_length=max_len_txt)\n",
    "    tokenized_label = tokenizer(batch['title'], padding='max_length', truncation=True, max_length=max_len_tlt)\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "\n",
    "    return tokenized_input\n",
    "\n",
    "dataset_train = dataset_train.map(tokenize, batched=True, batch_size=8)\n",
    "dataset_test = dataset_test.map(tokenize, batched=True, batch_size=8)\n",
    "\n",
    "dataset_train.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dataset_test.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.save_to_disk('lenta2/train')\n",
    "dataset_test.save_to_disk('lenta2/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 766/766 [00:00<00:00, 439kB/s]\n",
      "loading configuration file config.json from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"cointegrated/rut5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"bos_token_id\": 2,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 200,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 977M/977M [01:27<00:00, 11.2MB/s]\n",
      "loading weights file pytorch_model.bin from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at IlyaGusev/rut5_base_sum_gazeta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"IlyaGusev/rut5_base_sum_gazeta\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'lenta2/output'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=1000,\n",
    "    remove_unused_columns=True,\n",
    "    eval_steps=500, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 1600\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 400\n",
       " }))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_train = load_from_disk(\"lenta2/train\")\n",
    "dataset_test = load_from_disk(\"lenta2/test\")\n",
    "dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, text. If title, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2400\n",
      "  Number of trainable parameters = 244309248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='2400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/2400 3:34:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.497400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to lenta2/output/checkpoint-1000\n",
      "Configuration saved in lenta2/output/checkpoint-1000/config.json\n",
      "Model weights saved in lenta2/output/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to lenta2/output/checkpoint-2000\n",
      "Configuration saved in lenta2/output/checkpoint-2000/config.json\n",
      "Model weights saved in lenta2/output/checkpoint-2000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2400, training_loss=1.9987492879231772, metrics={'train_runtime': 12905.7855, 'train_samples_per_second': 0.372, 'train_steps_per_second': 0.186, 'total_flos': 2549012889600000.0, 'train_loss': 1.9987492879231772, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to lenta2/output/model\n",
      "Configuration saved in lenta2/output/model/config.json\n",
      "Model weights saved in lenta2/output/model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_pred(idx):\n",
    "    input_text = dataset_test['text'][idx]\n",
    "    input_title = dataset_test['title'][idx]\n",
    "\n",
    "    use_cuda = False\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokenized_text = tokenizer(input_text, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "        source_ids = tokenized_text['input_ids'].to(dtype = torch.long)\n",
    "        source_mask = tokenized_text['attention_mask'].to(dtype = torch.long)\n",
    "        generated_ids = model.generate(\n",
    "            input_ids = source_ids,\n",
    "            attention_mask = source_mask, \n",
    "            max_length=1512,\n",
    "            num_beams=7,\n",
    "            temperature = 1.3,\n",
    "            repetition_penalty=1, \n",
    "            length_penalty=1, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "            ).to(device)\n",
    "        pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    print(\"Text:\\n\" + input_text)\n",
    "    print(\"Real title: \" + input_title)\n",
    "    print(\"Pred title: \" + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "–ú—ç—Ä—É –ú–æ—Å–∫–≤—ã –Æ—Ä–∏—é –õ—É–∂–∫–æ–≤—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Å—Ç–∞–≤–∫–∏ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –≤ –†–æ—Å—Å–∏–∏ –¥–æ—Å—Ä–æ—á–Ω—ã—Ö –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç—Å–∫–∏—Ö –≤—ã–±–æ—Ä–æ–≤ –∫–∞–∂–µ—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∞–ª—å–Ω–æ–π. –ö–∞–∫ —Å–æ–æ–±—â–∞–µ—Ç –ò–¢–ê–†-–¢–ê–°–°, –º—ç—Ä –∑–∞—è–≤–∏–ª, —á—Ç–æ –≥–ª–∞–≤–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –Ω–µ –º–æ–∂–µ—Ç \"–ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—é –∑–¥–æ—Ä–æ–≤—å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–≤–æ–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏\". –í –ø—Ä–∏–Ω—Ü–∏–ø–µ, —Å—á–∏—Ç–∞–µ—Ç –õ—É–∂–∫–æ–≤, –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –¥–æ—Å—Ä–æ—á–Ω—ã—Ö –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç—Å–∫–∏—Ö –≤—ã–±–æ—Ä–æ–≤ –≤–ø–æ–ª–Ω–µ –ø—Ä–∏–µ–º–ª–µ–º. –û–¥–Ω–∞–∫–æ –∏—Ö —Å–æ–≤–º–µ—â–µ–Ω–∏–µ –ø–æ —Å—Ä–æ–∫–∞–º —Å –≤—ã–±–æ—Ä–∞–º–∏ –≤ –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—É—é –î—É–º—É –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –∏–∑–±–∏—Ä–∞—Ç–µ–ª–µ–π \"–Ω–µ–ª—å–∑—è –ª–∏—à–∞—Ç—å –ø—Ä–∞–≤–∞ —Å–¥–µ–ª–∞—Ç—å –≤—ã–±–æ—Ä –∏–∑ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–≥–æ –∫—Ä—É–≥–∞ –ø—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç–æ–≤\". –í—ã–≤–æ–¥ –∏–∑ –∏–≥—Ä—ã \"–Ω–µ—É–≥–æ–¥–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\" –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω, –æ—Ç–º–µ—Ç–∏–ª –Æ—Ä–∏–π –õ—É–∂–∫–æ–≤. –ù–∞–æ–±–æ—Ä–æ—Ç, –≤ –≤—ã–±–æ—Ä–∞—Ö –≥–ª–∞–≤—ã –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—á–∞—Å—Ç–∏–µ –≤—Å–µ—Ö –ø—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç–æ–≤. \"–í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –Ω–∞—Ä–æ–¥ –Ω–µ –±—É–¥–µ—Ç –∏–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–¥–µ–ª–∞—Ç—å –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä\", - –¥–æ–±–∞–≤–∏–ª –æ–Ω. –ú—ç—Ä –ú–æ—Å–∫–≤—ã —É–≤–µ—Ä–µ–Ω: –Ω–∞–∏–±–æ–ª–µ–µ –¥–æ—Å—Ç–æ–π–Ω–æ–π –∫–∞–Ω–¥–∏–¥–∞—Ç—É—Ä–æ–π –Ω–∞ –ø–æ—Å—Ç –Ω–æ–≤–æ–≥–æ –≥–ª–∞–≤—ã —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –º–æ–≥ –±—ã —Å—Ç–∞—Ç—å –ï–≤–≥–µ–Ω–∏–π –ü—Ä–∏–º–∞–∫–æ–≤. –í —ç—Ç–æ–π —Å–≤—è–∑–∏ –Æ—Ä–∏–π –õ—É–∂–∫–æ–≤ –µ—â–µ —Ä–∞–∑ –ø–æ–¥—á–µ—Ä–∫–Ω—É–ª, —á—Ç–æ –Ω–µ —Å—Ç–∞–Ω–µ—Ç –≤—ã–¥–≤–∏–≥–∞—Ç—å —Å–≤–æ—é –∫–∞–Ω–¥–∏–¥–∞—Ç—É—Ä—É, –µ—Å–ª–∏ –µ–º—É —É–¥–∞—Å—Ç—Å—è —É–≥–æ–≤–æ—Ä–∏—Ç—å –ï–≤–≥–µ–Ω–∏—è –ü—Ä–∏–º–∞–∫–æ–≤–∞ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ—Ç —à–∞–≥.\n",
      "Real title: –Æ—Ä–∏–π –õ—É–∂–∫–æ–≤: –æ—Ç—Å—Ç–∞–≤–∫–∞ –ï–ª—å—Ü–∏–Ω–∞ \"–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∞–ª—å–Ω–∞\"\n",
      "Pred title: –Æ—Ä–∏–π –õ—É–∂–∫–æ–≤ –Ω–µ –±—É–¥–µ—Ç –≤—ã–¥–≤–∏–≥–∞—Ç—å —Å–≤–æ—é –∫–∞–Ω–¥–∏–¥–∞—Ç—É—Ä—É\n"
     ]
    }
   ],
   "source": [
    "title_pred(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –≤—ã—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—é —Å–º–µ—Ä—Ç–µ–ª—å–Ω–æ–æ–ø–∞—Å–Ω—ã—Ö –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞ –¥—Ä–µ–≤–Ω–∏—Ö –≤–∏—Ä—É—Å–æ–≤, –Ω–∞—Ö–æ–¥—è—â–∏—Ö—Å—è –≤ \"–ª–µ–¥—è–Ω—ã—Ö—Å–∞—Ä–∫–æ—Ñ–∞–≥–∞—Ö\" –ø–æ–ª—é—Å–æ–≤ –ó–µ–º–ª–∏. –° —Ç–∞–∫–∏–º –∞–ø–æ–∫–∞–ª–∏–ø—Ç–∏—á–µ—Å–∫–∏–º–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –≤—ã—Å—Ç—É–ø–∏–ª —Å–µ–≥–æ–¥–Ω—è –∂—É—Ä–Ω–∞–ª \"New Scientist\",–æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –≤—ã–≤–æ–¥–∞—Ö —É—á–µ–Ω—ã—Ö, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –Ω–µ–¥–∞–≤–Ω–µ–π –Ω–∞—Ö–æ–¥–∫–æ–π–≤–æ –ª—å–¥–∞—Ö –ì—Ä–µ–Ω–ª–∞–Ω–¥–∏–∏ –¥—Ä–µ–≤–Ω–µ–π—à–µ–≥–æ –≤–∏—Ä—É—Å–∞. –¢–∞—è–Ω–∏–µ –ª—å–¥–æ–≤, –∫–∞–∫ —è–≤—Å—Ç–≤—É–µ—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è, –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–æ—è–≤–ª–µ–Ω–∏—é –≤ –∞—Ç–º–æ—Å—Ñ–µ—Ä–µ –∏ –º–∏—Ä–æ–≤–æ–º –æ–∫–µ–∞–Ω–µ –¥—Ä–µ–≤–Ω–∏—Ö —à—Ç–∞–º–º–æ–≤ –æ—Å–ø—ã, –ø–æ–ª–∏–æ–º–∏–µ–ª–∏—Ç–∞ –∏ –≥—Ä–∏–ø–ø–∞, –æ—Ç –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç \"–ø—Ä–æ—Ç–∏–≤–æ—è–¥–∏—è\". –†–∞–∑–Ω–æ—Å–∏–º—ã–µ –≤–µ—Ç—Ä–æ–º –∏ —Ç–µ—á–µ–Ω–∏—è–º–∏, –æ–Ω–∏ –º–æ–≥—É—Ç –≤—ã–∑–≤–∞—Ç—å –Ω–∞ –ó–µ–º–ª–µ –ø–æ–≤–∞–ª—å–Ω—ã–µ —ç–ø–∏–¥–µ–º–∏–∏ –∏ —Ö–∞–æ—Å. –ì—Ä—É–ø–ø–∞ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —É—á–µ–Ω—ã—Ö –∏–∑ –°–∏—Ä–∞–∫—É–∑—Å–∫–æ–≥–æ –∏–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –≤ –ù—å—é-–ô–æ—Ä–∫–µ, –ø–æ–≤–µ–¥–∞–≤—à–∏—Ö –æ —Å–≤–æ–µ–π–Ω–∞—Ö–æ–¥–∫–µ –±—Ä–∏—Ç–∞–Ω—Å–∫–æ–º—É –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–∏–∫—É, —Å–æ–æ–±—â–∏–ª–∞, —á—Ç–æ –≤–æ–∑—Ä–∞—Å—Ç–∏—Å—Å–ª–µ–¥—É–µ–º—ã—Ö –ª—å–¥–æ–≤ –≤ –ì—Ä–µ–Ω–ª–∞–Ω–¥–∏–∏ –∫–æ–ª–µ–±–∞–ª—Å—è –æ—Ç 500 –ª–µ—Ç –¥–æ 140 —Ç—ã—Å—è—á–ª–µ—Ç. –í–∏—Ä—É—Å, –ø–æ —Å–ª–æ–≤–∞–º —É—á–µ–Ω—ã—Ö, –Ω–∞–¥–µ–∂–Ω–æ –∑–∞—â–∏—â–∞–µ—Ç –ø—Ä–æ—Ç–µ–∏–Ω–æ–≤–∞—è–æ–±–æ–ª–æ—á–∫–∞, –∏ –ø—Ä–∏ –±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –æ–Ω –≤–ø–æ–ª–Ω–µ –º–æ–∂–µ—Ç \"–∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å\".–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –Ω–∞ –ó–µ–º–ª–µ –º–æ–∂–µ—Ç –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –¥—Ä—É–≥–∏–µ –≤–∏—Ä—É—Å—ã, –∏–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è —ç—Ç–æ–≥–æ –±—É–¥—É—Ç –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–º–∏. –ü–æ –¥–∞–Ω–Ω—ã–º –æ–¥–Ω–æ–≥–æ –∏–∑ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π -—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞-–≤–∏—Ä—É—Å–æ–ª–æ–≥–∞ –≠–ª–≤–∏–Ω–∞ –°–º–∏—Ç–∞, –µ—Å–ª–∏ –≤–∏—Ä—É—Å—ã –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å –≤\"–±–µ–∑–¥–µ–π—Å—Ç–≤–∏–∏\" —Ç—ã—Å—è—á–∏ –ª–µ—Ç, –Ω–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—è —Å –æ–±—ä–µ–∫—Ç–æ–º, —Ç–æ–≤–ø–æ–ª–Ω–µ –º–æ–≥—É—Ç —Å—Ç–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º —ç–ø–∏–¥–µ–º–∏–π. –°–º–∏—Ç —É–∂–µ –ø–æ–ª—É—á–∏–ª —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–æ–≥–æ, —á—Ç–æ –≤–∏—Ä—É—Å—ã, –≤—ã–∑—ã–≤–∞—é—â–∏–µ–æ—Å—Ç—Ä–æ–µ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∂–µ–ª—É–¥–∫–∞, –≤—Ä–µ–º—è –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—è–≤–ª—è—é—Ç—Å—è –∏–∑–≥–ª—É–±–∏–Ω –æ–∫–µ–∞–Ω–∞.\n",
      "Real title: –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ç–∞—è–Ω–∏–µ –ª—å–¥–æ–≤ –≤—ã—Å–≤–æ–±–æ–¥–∏—Ç –¥—Ä–µ–≤–Ω–∏–µ –≤–∏—Ä—É—Å—ã\n",
      "Pred title: –í –ì—Ä–µ–Ω–ª–∞–Ω–¥–∏–∏ –Ω–∞–π–¥–µ–Ω—ã –¥—Ä–µ–≤–Ω–∏–µ –≤–∏—Ä—É—Å—ã\n"
     ]
    }
   ],
   "source": [
    "title_pred(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "–ú–µ—Ä—ã –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —á–∞—Å—Ç–Ω—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö —Å–µ—Ç–µ–π, –ø—Ä–∏–Ω–∏–º–∞–µ–º—ã–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–µ–π –ö–ª–∏–Ω—Ç–æ–Ω–∞, —è–≤–Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã –¥–ª—è –ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ—è–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —É–≥—Ä–æ–∑–µ –≤–∑–ª–æ–º–æ–≤. –û–± —ç—Ç–æ–º –ø—Ä–æ–∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∏ —ç–∫—Å–ø–µ—Ä—Ç—ã –∫–æ–Ω–≥—Ä–µ—Å—Å–∞ –°–®–ê, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–≤—à–∏–µ –≤—á–µ—Ä–∞ –¥–æ–∫–ª–∞–¥ –≤ –ö–æ–Ω–≥—Ä–µ—Å—Å, —Å–æ–æ–±—â–∞–µ—Ç Reuters. –î–æ–∫–ª–∞–¥ –±—ã–ª –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –ø–æ –∑–∞–ø—Ä–æ—Å—É –†–æ–±–µ—Ä—Ç–∞ –ë–µ–Ω–Ω–µ—Ç–∞, –≤–æ–∑–≥–ª–∞–≤–ª—è—é—â–µ–≥–æ –∫–æ–º–∏—Ç–µ—Ç –ö–æ–Ω–≥—Ä–µ—Å—Å–∞ –ø–æ \"–ø—Ä–æ–±–ª–µ–º–µ-2000\", –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—è —Ä–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–æ–π –ø–∞—Ä—Ç–∏–∏ –æ—Ç —à—Ç–∞—Ç–∞ –Æ—Ç–∞. –°–æ–≥–ª–∞—Å–Ω–æ –∏–∑–ª–æ–∂–µ–Ω–Ω—ã–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –¥–∞–Ω–Ω—ã–º, —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫ –≤–∑–ª–æ–º–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö —Å–µ—Ç–µ–π –≤–æ–∑—Ä–æ—Å–ª–æ —Å 1334 –≤ 1993 –≥–æ–¥—É –¥–æ 4398 –≤ –ø–µ—Ä–≤–æ–π –ø–æ–ª–æ–≤–∏–Ω–µ 1999 –≥–æ–¥–∞ (—á—Ç–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –ø–æ—Å–∫–æ–ª—å–∫—É —á–∏—Å–ª–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∫ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤ –∏ —Å–µ—Ç–µ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è). –ü–æ –º–Ω–µ–Ω–∏—é —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —Ç–∞–∫–∏—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –≤–∏—Ä—É—Å–æ–≤ –∫–∞–∫ \"–ú–µ–ª–∏—Å—Å–∞\" (–Ω–∞—á–∞–ª–æ 1999 –≥–æ–¥–∞) –∏ —Ö–∞–∫–µ—Ä—Å–∫–∞—è –∞—Ç–∞–∫–∞ –Ω–∞ –ú–∏–Ω–æ–±–æ—Ä–æ–Ω—ã –°–®–ê (—Ñ–µ–≤—Ä–∞–ª—å 1998) –≤–ø–æ–ª–Ω–µ –æ—Ç—Ä–∞–∂–∞—é—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö —Å–µ—Ç–µ–π. –ï—â–µ –±–æ–ª—å—à–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Ç—Ä–µ–≤–æ–∂–∞—Ç —á–∞—Å—Ç–Ω—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ —Å–µ—Ç–∏, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—â–∏–µ —ç–Ω–µ—Ä–≥–µ—Ç–∏–∫—É, —Ç–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏, —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω–æ –≤–∞–∂–Ω—ã–µ —Å—Ñ–µ—Ä—ã, –Ω–µ –±—É–¥—É—á–∏ –ø—Ä–∏ —ç—Ç–æ–º –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –∑–∞—â–∏—â–µ–Ω–Ω—ã–º–∏.\n",
      "Real title: –ö–æ–Ω–≥—Ä–µ—Å—Å –°–®–ê –æ–±–µ—Å–ø–æ–∫–æ–µ–Ω —É—Ä–æ–≤–Ω–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n",
      "Pred title: –°–®–ê —É–≥—Ä–æ–∂–∞–µ—Ç –≤–∑–ª–æ–º–∞–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö —Å–µ—Ç–µ–π\n"
     ]
    }
   ],
   "source": [
    "title_pred(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "3 –æ–∫—Ç—è–±—Ä—è –±—Ä–∏—Ç–∞–Ω—Å–∫–∞—è –≥–∞–∑–µ—Ç–∞ The Sunday Times —Å–æ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ä–∞–∑–≤–µ–¥–∫–µ —Å–æ–æ–±—â–∏–ª–∞, —á—Ç–æ —Å–∏–ª—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –æ–∂–∏–¥–∞—é—Ç –ø—Ä–∏–∫–∞–∑–∞ –æ –Ω–∞—á–∞–ª–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º—É —É–Ω–∏—á—Ç–æ–∂–µ–Ω–∏—é –®–∞–º–∏–ª—è –ë–∞—Å–∞–µ–≤–∞ –∏ –•–∞—Ç—Ç–∞–±–∞. –ö–∞–∫ —É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –≥–∞–∑–µ—Ç–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–≤–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è —É–Ω–∏—á—Ç–æ–∂–µ–Ω–∏—è –ª–∏–¥–µ—Ä–æ–≤ –±–æ–µ–≤–∏–∫–æ–≤. –°–æ–≥–ª–∞—Å–Ω–æ –ø–µ—Ä–≤–æ–º—É, –≤–æ–π—Å–∫–∞ –ë–∞—Å–∞–µ–≤–∞ –∏ –•–∞—Ç—Ç–∞–±–∞ –±—É–¥—É—Ç –≤–∑—è—Ç—ã –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã—Ö —Å–∏–ª –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ 50,000 —á–µ–ª–æ–≤–µ–∫, –ø–æ—Å–ª–µ —á–µ–≥–æ –ø–æ —Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∞–º –±—É–¥—É—Ç –Ω–∞–Ω–µ—Å–µ–Ω—ã –±–æ–º–±–æ–≤—ã–µ —É–¥–∞—Ä—ã —Å –≤–æ–∑–¥—É—Ö–∞. –ü–æ –¥—Ä—É–≥–æ–º—É –ø–ª–∞–Ω—É, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞–Ω–µ—Å—Ç–∏ —Ä–∞–∫–µ—Ç–Ω—ã–π —É–¥–∞—Ä –ø–æ –ë–∞—Å–∞–µ–≤—É, –∏—Å–ø–æ–ª—å–∑—É—è –¥–ª—è –Ω–∞–≤–µ–¥–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª, –∏–∑–ª—É—á–∞–µ–º—ã–π –µ–≥–æ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–º —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º. –ü–æ–¥–æ–±–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≤ 1996 –≥–æ–¥—É –±—ã–ª —É–Ω–∏—á—Ç–æ–∂–µ–Ω –±—ã–≤—à–∏–π –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –ß–µ—á–Ω–∏ –î–∂–æ—Ö–∞—Ä –î—É–¥–∞–µ–≤. –ü–æ —Å–ª–æ–≤–∞–º –≥–∞–∑–µ—Ç—ã, –ø—Ä–∏–∫–∞–∑ –¥–æ–ª–∂–µ–Ω –ø–æ—Å—Ç—É–ø–∏—Ç—å –æ—Ç –ø—Ä–µ–º—å–µ—Ä-–º–∏–Ω–∏—Å—Ç—Ä–∞ –†–§ –í–ª–∞–¥–∏–º–∏—Ä–∞ –ü—É—Ç–∏–Ω–∞. –í –º–∏–Ω—É–≤—à–µ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ –≤ –∏–Ω—Ç–µ—Ä–≤—å—é —Ä–∞–¥–∏–æ—Å—Ç–∞–Ω—Ü–∏–∏ \"–≠—Ö–æ –ú–æ—Å–∫–≤—ã\" –±—ã–≤—à–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø—Ä–µ–º—å–µ—Ä –°–µ—Ä–≥–µ–π –°—Ç–µ–ø–∞—à–∏–Ω –≤—ã—Å–∫–∞–∑–∞–ª—Å—è –∑–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–µ–≤—ã—Ö –∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤ –®–∞–º–∏–ª—è –ë–∞—Å–∞–µ–≤–∞ –∏ –•–∞—Ç—Ç–∞–±–∞. –ö–æ–º–º–µ–Ω—Ç–∏—Ä—É—è —Å–æ–æ–±—â–µ–Ω–∏–µ The Sunday Times, –°—Ç–µ–ø–∞—à–∏–Ω –∑–∞—è–≤–∏–ª, —á—Ç–æ \"–•–∞—Ç—Ç–∞–±, –ë–∞—Å–∞–µ–≤ –∏ –µ—â–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∞–Ω–¥–∏—Ç–æ–≤, —É–±–∏–π—Ü, —Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–æ–≤ –Ω–µ –¥–æ–ª–∂–Ω—ã –∂–∏—Ç—å –Ω–∞ –∑–µ–º–ª–µ –≤–æ–æ–±—â–µ\". –û–¥–Ω–∞–∫–æ, –∑–∞–º–µ—Ç–∏–ª –æ–Ω, —Ç–∞–∫–∏–µ —Ç–µ–º—ã –Ω–µ –Ω–∞–¥–æ –æ–±—Å—É–∂–¥–∞—Ç—å –≤ –°–ú–ò, –ø–æ—Å–∫–æ–ª—å–∫—É \"—ç—Ç–æ —Ç–æ–Ω–∫–∞—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞\", –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ —É–∑–Ω–∞–≤–∞—Ç—å –ø–æ –∏—Ç–æ–≥–∞–º –æ–ø–µ—Ä–∞—Ü–∏–∏.\n",
      "Real title: –°—Ç–µ–ø–∞—à–∏–Ω –∏¬†—Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —Å–ø–µ—Ü—Å–ª—É–∂–±—ã —Ö–æ—Ç—è—Ç —É—Å—Ç—Ä–∞–Ω–∏—Ç—å –ë–∞—Å–∞–µ–≤–∞ —Å¬†–•–∞—Ç—Ç–∞–±–æ–º\n",
      "Pred title: –£–Ω–∏—á—Ç–æ–∂–µ–Ω—ã –®–∞–º–∏–ª—å –ë–∞—Å–∞–µ–≤ –∏ –•–∞—Ç—Ç–∞–±\n"
     ]
    }
   ],
   "source": [
    "title_pred(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gJABxhalLVQu",
    "IaQMCGHFLVQ6",
    "5AJk1B39LVRP",
    "RJlvqWuALVRs",
    "rck5OVqhLVSA",
    "mV3fmzp-LVSU",
    "H5THCOjMLVSg",
    "02s2Vh7MLVSj",
    "b1khxRFDLVSm",
    "sfUmWcAQLVSt",
    "BxvtN-3zLVS5",
    "gyrHhYkgLVTB"
   ],
   "name": "sem1_intro_common.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
