{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 14. Transfer learning\n",
    "\n",
    "  1. взять данные из\n",
    "    https://www.kaggle.com/datasets/mrapplexz/bashim-quotes \\\n",
    "    обучить модель GPT для генерации своих цитат\n",
    "\n",
    "  2. взять новостные данные из\n",
    "    https://github.com/natasha/corus \\\n",
    "    load_lenta2 \\\n",
    "    нам понадобиться сам текст и заголовок \\\n",
    "    обучить модель T5/ или GPT для генерации заголовков для статей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers corus sentencepiece -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-08 12:38:26.101376: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!unzip \"data/archive.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-30 11:24:00+00:00</td>\n",
       "      <td>22010.0</td>\n",
       "      <td>&lt;Ares&gt; ppdv, все юниксы очень дружелюбны.. они...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-30 11:25:00+00:00</td>\n",
       "      <td>25105.0</td>\n",
       "      <td>&lt;томатик_рад&gt; а ты не чувствуешь красоту мира?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-30 11:27:00+00:00</td>\n",
       "      <td>7192.0</td>\n",
       "      <td>&lt;Дор&gt; \"мышка, почему у тебя такие большие глаз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-30 11:28:00+00:00</td>\n",
       "      <td>29169.0</td>\n",
       "      <td>&lt;PPDV[os2]&gt; \"Мальчики, вы что больные, бегать ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004-08-30 11:26:00+00:00</td>\n",
       "      <td>7140.0</td>\n",
       "      <td>&lt;Ohtori_Akio&gt; мы - как разработчики - живём с ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date   rating  \\\n",
       "id                                      \n",
       "1  2004-08-30 11:24:00+00:00  22010.0   \n",
       "2  2004-08-30 11:25:00+00:00  25105.0   \n",
       "3  2004-08-30 11:27:00+00:00   7192.0   \n",
       "4  2004-08-30 11:28:00+00:00  29169.0   \n",
       "5  2004-08-30 11:26:00+00:00   7140.0   \n",
       "\n",
       "                                                 text  \n",
       "id                                                     \n",
       "1   <Ares> ppdv, все юниксы очень дружелюбны.. они...  \n",
       "2   <томатик_рад> а ты не чувствуешь красоту мира?...  \n",
       "3   <Дор> \"мышка, почему у тебя такие большие глаз...  \n",
       "4   <PPDV[os2]> \"Мальчики, вы что больные, бегать ...  \n",
       "5   <Ohtori_Akio> мы - как разработчики - живём с ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATASET_PATH) as f:\n",
    "     df = pd.read_json(DATASET_PATH, lines=True).set_index('id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Ares&gt; ppdv, все юниксы очень дружелюбны.. они...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;томатик_рад&gt; а ты не чувствуешь красоту мира?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;Дор&gt; \"мышка, почему у тебя такие большие глаз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PPDV[os2]&gt; \"Мальчики, вы что больные, бегать ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;Ohtori_Akio&gt; мы - как разработчики - живём с ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "id                                                   \n",
       "1   <Ares> ppdv, все юниксы очень дружелюбны.. они...\n",
       "2   <томатик_рад> а ты не чувствуешь красоту мира?...\n",
       "3   <Дор> \"мышка, почему у тебя такие большие глаз...\n",
       "4   <PPDV[os2]> \"Мальчики, вы что больные, бегать ...\n",
       "5   <Ohtori_Akio> мы - как разработчики - живём с ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['date', 'rating'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 608/608 [00:00<00:00, 136kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.71M/1.71M [00:00<00:00, 2.06MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.27M/1.27M [00:00<00:00, 1.54MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading: 100%|███████████████████████████| 551M/551M [00:48<00:00, 11.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***\n",
      "xxx: Чо как?\n",
      "yyy: Да вот, на работе закрыли доступ к вк, сижу теперь на hh\n",
      "***\n",
      "<Пинг> бля, мне даже в sims2 не дают!\n",
      "***\n",
      "xxx: В армии ж как: круглое - несут, а квадратное - катят. \n",
      "xxx: Вот стою и смотрю на снеговика в нашей части. Квадратный. \n",
      "xxx: Все правильно - накатали.\n",
      "***\n",
      "< mary.j > одела я сегодня свою чёрную кофточку с декольте, ну и иду платить за кредит. Иду вниз на саксаганского стоят 3 парня и лупятся на мои сиськт :\"вот это да\", \"круто\"... Иду наверх они же и то же самое. Я \"мальчики, вы что сисек не видили?\" . Один из них : \"да обычно либо сиськи либо лицо. А Вам так к лицу Ваши сиськи\".\n",
      "***\n",
      "- аватар у тебя клевый :)\n",
      "- ага!!! а что это?\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sep = '\\n***\\n'\n",
    "prefix = sep.join([''] + random.sample(list(df['text']), k=5) + [''])\n",
    "tokens = tokenizer(prefix, return_tensors='pt')\n",
    "tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "end_token_id = tokenizer.encode('***')[0]\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "size = tokens['input_ids'].shape[1]\n",
    "output = model.generate(\n",
    "    **tokens, \n",
    "    do_sample=False, \n",
    "    max_length=size+50, \n",
    "    repetition_penalty=5., \n",
    "    temperature=0.5,\n",
    "    num_beams=10,\n",
    ")\n",
    "decoded = tokenizer.decode(output[0])\n",
    "result = decoded[len(prefix):]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df.loc[:10000, 'text'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def build_text_files(data_json, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    for texts in data_json:\n",
    "        summary = str(texts).strip()\n",
    "        summary = re.sub(r\"\\[\\w+\\]\", \"\", summary)\n",
    "        summary = re.sub(r\"<[\\w+,\\!, -]>\", \"\", summary)\n",
    "        summary = re.sub(r\"<\\w+>\", \"\", summary)\n",
    "        summary = re.sub(r\"\\s\", \" \", summary)\n",
    "        data += summary + \"  \"\n",
    "    f.write(data)\n",
    "  \n",
    "build_text_files(train,'./train_dataset.txt')\n",
    "build_text_files(test,'./test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1666\n",
      "Test dataset length: 294\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset length: \"+ str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_path = './train_dataset.txt'\n",
    "test_path = './test_dataset.txt'\n",
    "\n",
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./GPT/gpt2-train\", \n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4,  \n",
    "    eval_steps = 400, \n",
    "    save_steps=800, \n",
    "    warmup_steps=500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 676\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 507\n",
      "  Number of trainable parameters = 125231616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [507/507 53:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.273200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=507, training_loss=4.267856063692292, metrics={'train_runtime': 3227.2923, 'train_samples_per_second': 0.628, 'train_steps_per_second': 0.157, 'total_flos': 132475060224000.0, 'train_loss': 4.267856063692292, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./GPT/gpt2-train\n",
      "Configuration saved in ./GPT/gpt2-train/config.json\n",
      "Model weights saved in ./GPT/gpt2-train/pytorch_model.bin\n",
      "tokenizer config file saved in GPT/gpt2-train/tokenizer_config.json\n",
      "Special tokens file saved in GPT/gpt2-train/special_tokens_map.json\n",
      "Configuration saved in GPT/model_gpt2/config.json\n",
      "Model weights saved in GPT/model_gpt2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained('GPT/gpt2-train')\n",
    "model.save_pretrained('GPT/model_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file GPT/model_gpt2/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"GPT/model_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file GPT/model_gpt2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at GPT/model_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GPT/gpt2-train\")\n",
    "model_new = AutoModelForCausalLM.from_pretrained(\"GPT/model_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ia http://www.livejournal.com/users/maxim_korolev/126565.html#cutid1  <@Luftwaffe> У меня есть подозрение, что кто-то очень хочет со мной поговорить по душам... (с) * Viktor has quit IRC (Viktor is now known as viktor_officer) [Quit]\n"
     ]
    }
   ],
   "source": [
    "size = tokens['input_ids'].shape[1]\n",
    "output = model_new.generate(\n",
    "    **tokens, \n",
    "    do_sample=False, \n",
    "    max_length=size+100, \n",
    "    repetition_penalty=5., \n",
    "    temperature=0.5,\n",
    "    num_beams=10,\n",
    ")\n",
    "decoded = tokenizer.decode(output[0])\n",
    "result = decoded[len(prefix):]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LentaRecord(\n",
       "    url='https://lenta.ru/news/1914/09/16/hungarnn/',\n",
       "    title='1914. Русские войска вступили в\\xa0пределы Венгрии  ',\n",
       "    text='Бои у Сопоцкина и Друскеник закончились отступлением германцев. Неприятель, приблизившись с севера к Осовцу начал артиллерийскую борьбу с крепостью. В артиллерийском бою принимают участие тяжелые калибры. С раннего утра 14 сентября огонь достиг значительного напряжения. Попытка германской пехоты пробиться ближе к крепости отражена. В Галиции мы заняли Дембицу. Большая колонна, отступавшая по шоссе от Перемышля к Саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. Вылазки гарнизона Перемышля остаются безуспешными. При продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. На перевале Ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы Венгрии. «Русский инвалид», 16 сентября 1914 года.',\n",
       "    topic='Библиотека',\n",
       "    tags='Первая мировая',\n",
       "    date=datetime.datetime(1914, 9, 16, 0, 0)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from corus import load_lenta2\n",
    "\n",
    "path = 'data/lenta-ru-news.csv.bz2'\n",
    "records = load_lenta2(path)\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
       "      <td>Министерство народного просвещения, в виду про...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1914. Das ist Nesteroff!</td>\n",
       "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1914. Бульдог-гонец под Льежем</td>\n",
       "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
       "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Космонавты сомневаются в надежности \"Мира\"</td>\n",
       "      <td>Как стало известно агентству Ассошиэйтед Пресс...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  1914. Празднование столетия М.Ю. Лермонтова от...   \n",
       "1                           1914. Das ist Nesteroff!   \n",
       "2                    1914. Бульдог-гонец под Льежем    \n",
       "3           1914. Под Люблином пойман швабский зверь   \n",
       "4         Космонавты сомневаются в надежности \"Мира\"   \n",
       "\n",
       "                                                text  \n",
       "0  Министерство народного просвещения, в виду про...  \n",
       "1  Штабс-капитан П. Н. Нестеров на днях, увидев в...  \n",
       "2  Фотограф-корреспондент Daily Mirror рассказыва...  \n",
       "3  Лица, приехавшие в Варшаву из Люблина, передаю...  \n",
       "4  Как стало известно агентству Ассошиэйтед Пресс...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(record.title, record.text) for record in records]\n",
    "df_news = pd.DataFrame({'title': [record[0] for record in data], 'text': [record[1] for record in data]})\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800974, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_news[:2000], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['title', 'text'],\n",
       "     num_rows: 1600\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'text'],\n",
       "     num_rows: 400\n",
       " }))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Министр внутренних дел России Владимир Рушайло считает, что чеченский муфтий Ахмед Кадыров может стать \"альтернативной фигурой\" при переговорах между федеральным центром и Чечней по урегулированию ситуации в республике. Об этом он сообщил в четверг в беседе с журналистами, передает ИТАР-ТАСС. Глава МВД напомнил, что председатель правительства Владимир Путин и муфтий Кадыров встречались накануне и провели успешный диалог. Рушайло с удовлетворением отметил, что \"политический диалог уже начался\", однако он подчеркнул, что \"мы пока не делаем далеко идущих выводов\". Рушайло в очередной раз подчеркнул, что Аслан Масхадов не может представлять чеченскую сторону на переговорах. Напомним, что 11 октября Масхадов заявил, что он освобождает от должности муфтия чеченской республики Ахмеда-Хаджи Кадырова. Масхадов объяснил тогда свое решение (весьма сомнительное с точки зрения шариата) тем, что муфтий \"пытается развязать гражданскую войну в Чечне\".'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/spiece.model\n",
      "loading file tokenizer.json from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/tokenizer_config.json\n",
      "100%|████████████████████████████████████████| 200/200 [00:01<00:00, 145.75ba/s]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 143.74ba/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"IlyaGusev/rut5_base_sum_gazeta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_len_text = max(map(lambda txt: len(txt.split()), dataset_train['text']))\n",
    "max_len_tlt = max(map(lambda txt: len(txt.split()), dataset_train['title']))\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized_input = tokenizer(batch['text'], padding='max_length', truncation=True, max_length=max_len_txt)\n",
    "    tokenized_label = tokenizer(batch['title'], padding='max_length', truncation=True, max_length=max_len_tlt)\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "\n",
    "    return tokenized_input\n",
    "\n",
    "dataset_train = dataset_train.map(tokenize, batched=True, batch_size=8)\n",
    "dataset_test = dataset_test.map(tokenize, batched=True, batch_size=8)\n",
    "\n",
    "dataset_train.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dataset_test.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.save_to_disk('lenta2/train')\n",
    "dataset_test.save_to_disk('lenta2/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 766/766 [00:00<00:00, 439kB/s]\n",
      "loading configuration file config.json from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"cointegrated/rut5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"bos_token_id\": 2,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 200,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "Downloading: 100%|███████████████████████████| 977M/977M [01:27<00:00, 11.2MB/s]\n",
      "loading weights file pytorch_model.bin from cache at /Users/dv/.cache/huggingface/hub/models--IlyaGusev--rut5_base_sum_gazeta/snapshots/f09a08cae5d74c70e55da1a6ebb49f88c26f433b/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at IlyaGusev/rut5_base_sum_gazeta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"IlyaGusev/rut5_base_sum_gazeta\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'lenta2/output'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=1000,\n",
    "    remove_unused_columns=True,\n",
    "    eval_steps=500, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 1600\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 400\n",
       " }))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_train = load_from_disk(\"lenta2/train\")\n",
    "dataset_test = load_from_disk(\"lenta2/test\")\n",
    "dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, text. If title, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2400\n",
      "  Number of trainable parameters = 244309248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='2400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/2400 3:34:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.497400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to lenta2/output/checkpoint-1000\n",
      "Configuration saved in lenta2/output/checkpoint-1000/config.json\n",
      "Model weights saved in lenta2/output/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to lenta2/output/checkpoint-2000\n",
      "Configuration saved in lenta2/output/checkpoint-2000/config.json\n",
      "Model weights saved in lenta2/output/checkpoint-2000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2400, training_loss=1.9987492879231772, metrics={'train_runtime': 12905.7855, 'train_samples_per_second': 0.372, 'train_steps_per_second': 0.186, 'total_flos': 2549012889600000.0, 'train_loss': 1.9987492879231772, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to lenta2/output/model\n",
      "Configuration saved in lenta2/output/model/config.json\n",
      "Model weights saved in lenta2/output/model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_pred(idx):\n",
    "    input_text = dataset_test['text'][idx]\n",
    "    input_title = dataset_test['title'][idx]\n",
    "\n",
    "    use_cuda = False\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokenized_text = tokenizer(input_text, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "        source_ids = tokenized_text['input_ids'].to(dtype = torch.long)\n",
    "        source_mask = tokenized_text['attention_mask'].to(dtype = torch.long)\n",
    "        generated_ids = model.generate(\n",
    "            input_ids = source_ids,\n",
    "            attention_mask = source_mask, \n",
    "            max_length=1512,\n",
    "            num_beams=7,\n",
    "            temperature = 1.3,\n",
    "            repetition_penalty=1, \n",
    "            length_penalty=1, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "            ).to(device)\n",
    "        pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    print(\"Text:\\n\" + input_text)\n",
    "    print(\"Real title: \" + input_title)\n",
    "    print(\"Pred title: \" + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Мэру Москвы Юрию Лужкову вероятность отставки президента и проведения в России досрочных президентских выборов кажется достаточно реальной. Как сообщает ИТАР-ТАСС, мэр заявил, что глава российского государства не может \"по состоянию здоровья эффективно выполнять свои обязанности\". В принципе, считает Лужков, вариант проведения досрочных президентских выборов вполне приемлем. Однако их совмещение по срокам с выборами в Государственную Думу недопустимо, поскольку избирателей \"нельзя лишать права сделать выбор из более широкого круга претендентов\". Вывод из игры \"неугодных кандидатов\" невозможен, отметил Юрий Лужков. Наоборот, в выборах главы государства необходимо участие всех претендентов. \"В противном случае народ не будет иметь возможности сделать полноразмерный выбор\", - добавил он. Мэр Москвы уверен: наиболее достойной кандидатурой на пост нового главы российского государства мог бы стать Евгений Примаков. В этой связи Юрий Лужков еще раз подчеркнул, что не станет выдвигать свою кандидатуру, если ему удастся уговорить Евгения Примакова сделать этот шаг.\n",
      "Real title: Юрий Лужков: отставка Ельцина \"достаточно реальна\"\n",
      "Pred title: Юрий Лужков не будет выдвигать свою кандидатуру\n"
     ]
    }
   ],
   "source": [
    "title_pred(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Глобальное потепление приведет к высвобождению смертельноопасных для человечества древних вирусов, находящихся в \"ледяныхсаркофагах\" полюсов Земли. С таким апокалиптическимпредсказанием выступил сегодня журнал \"New Scientist\",основываясь на выводах ученых, подтвержденных недавней находкойво льдах Гренландии древнейшего вируса. Таяние льдов, как явствует из сообщения, может привести к появлению в атмосфере и мировом океане древних штаммов оспы, полиомиелита и гриппа, от которых нет \"противоядия\". Разносимые ветром и течениями, они могут вызвать на Земле повальные эпидемии и хаос. Группа американских ученых из Сиракузского иГосударственного университетов в Нью-Йорке, поведавших о своейнаходке британскому еженедельнику, сообщила, что возрастисследуемых льдов в Гренландии колебался от 500 лет до 140 тысячлет. Вирус, по словам ученых, надежно защищает протеиноваяоболочка, и при благоприятных условиях он вполне может \"заработать\".Специалисты утверждают, что даже незначительное повышениетемпературы на Земле может освободить другие вирусы, ипоследствия этого будут непредсказуемыми. По данным одного из американских исследователей -специалиста-вирусолога Элвина Смита, если вирусы находились в\"бездействии\" тысячи лет, не взаимодействуя с объектом, товполне могут стать источником эпидемий. Смит уже получил свидетельства того, что вирусы, вызывающиеострое расстройство желудка, время от времени появляются изглубин океана.\n",
      "Real title: Глобальное таяние льдов высвободит древние вирусы\n",
      "Pred title: В Гренландии найдены древние вирусы\n"
     ]
    }
   ],
   "source": [
    "title_pred(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Меры обеспечения безопасности правительственных и частных компьютерных сетей, принимаемые администрацией Клинтона, явно недостаточны для противостояния существующей угрозе взломов. Об этом проинформировали эксперты конгресса США, представившие вчера доклад в Конгресс, сообщает Reuters. Доклад был подготовлен по запросу Роберта Беннета, возглавляющего комитет Конгресса по \"проблеме-2000\", представителя республиканской партии от штата Юта. Согласно изложенным в документе данным, число попыток взлома компьютерных сетей возросло с 1334 в 1993 году до 4398 в первой половине 1999 года (что естественно, поскольку число подключенных к Интернету компьютеров и сетевых пользователей стремительно увеличивается). По мнению экспертов, распространение таких компьютерных вирусов как \"Мелисса\" (начало 1999 года) и хакерская атака на Минобороны США (февраль 1998) вполне отражают уязвимость существующих систем обеспечения безопасности компьютерных сетей. Еще больше экспертов тревожат частные компьютерные сети, контролирующие энергетику, телекоммуникации, финансовые транзакции, транспорт и многие другие жизненно важные сферы, не будучи при этом адекватно защищенными.\n",
      "Real title: Конгресс США обеспокоен уровнем компьютерной безопасности\n",
      "Pred title: США угрожает взломам компьютерных сетей\n"
     ]
    }
   ],
   "source": [
    "title_pred(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "3 октября британская газета The Sunday Times со ссылкой на источники в российской разведке сообщила, что силы специального назначения ожидают приказа о начале операции по физическому уничтожению Шамиля Басаева и Хаттаба. Как утверждает газета, разработаны два сценария уничтожения лидеров боевиков. Согласно первому, войска Басаева и Хаттаба будут взяты в окружение группировкой федеральных сил в количестве 50,000 человек, после чего по террористам будут нанесены бомбовые удары с воздуха. По другому плану, предполагается нанести ракетный удар по Басаеву, используя для наведения сигнал, излучаемый его спутниковым телефоном. Подобным образом в 1996 году был уничтожен бывший президент Чечни Джохар Дудаев. По словам газеты, приказ должен поступить от премьер-министра РФ Владимира Путина. В минувшее воскресенье в интервью радиостанции \"Эхо Москвы\" бывший российский премьер Сергей Степашин высказался за физическое устранение полевых командиров Шамиля Басаева и Хаттаба. Комментируя сообщение The Sunday Times, Степашин заявил, что \"Хаттаб, Басаев и еще несколько бандитов, убийц, террористов не должны жить на земле вообще\". Однако, заметил он, такие темы не надо обсуждать в СМИ, поскольку \"это тонкая оперативная работа\", о результатах которой нужно узнавать по итогам операции.\n",
      "Real title: Степашин и российские спецслужбы хотят устранить Басаева с Хаттабом\n",
      "Pred title: Уничтожены Шамиль Басаев и Хаттаб\n"
     ]
    }
   ],
   "source": [
    "title_pred(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gJABxhalLVQu",
    "IaQMCGHFLVQ6",
    "5AJk1B39LVRP",
    "RJlvqWuALVRs",
    "rck5OVqhLVSA",
    "mV3fmzp-LVSU",
    "H5THCOjMLVSg",
    "02s2Vh7MLVSj",
    "b1khxRFDLVSm",
    "sfUmWcAQLVSt",
    "BxvtN-3zLVS5",
    "gyrHhYkgLVTB"
   ],
   "name": "sem1_intro_common.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
